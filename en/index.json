[{"content":"1. 持久化怎么实现的 aof：append only file。持续写文件到buffer ring中，然后根据参数完成fsync操作。aof文件会有重写操作，节省空间，以及宕机恢复操作的时间。 rdb：内存快照。对某一时刻的redis的数据情况进行快照存储。 2. zset怎么做延迟队列 zadd key Val score：job数据存储在Val，score代表任务执行时间。 zrangebyscore time，拿当前的时间戳做比较。取可执行的任务。 zremrange 移除这些任务。\n3. 哪些操作会触发Redis单线程处理IO请求性能瓶颈。 耗时的操作包括：\n操作bigkey：写入一个bigkey在分配内存时需要消耗更多的时间，同样，删除bigkey释放内存同样会产生耗时\n使用复杂度过高的命令：例如SORT/SUNION/ZUNIONSTORE，或者O(N)命令，但是N很大，例如lrange key 0 -1一次查询全量数据\n大量key集中过期：Redis的过期机制也是在主线程中执行的，大量key集中过期会导致处理一个请求时，耗时都在删除过期key，耗时变长\n淘汰策略：淘汰策略也是在主线程执行的，当内存超过Redis内存上限后，每次写入都需要淘汰一些key，也会 造成耗时变长。\nAOF刷盘开启always机制：每次写入都需要把这个操作刷到磁盘，写磁盘的速度远比写内存慢，会拖慢Redis的性能\n主从全量同步生成RDB：虽然采用fork子进程生成数据快照，但fork这一瞬间也是会阻塞整个线程的，实例越大，阻塞时间越久\nRedis在6.0推出了多线程，可以在高并发场景下利用CPU多核多线程读写客户端数据，进一步提升server性能。当然，只针对客户端的读写是并行的，每个命令的真正操作依旧是单线程的\n4. redis分布式锁用过吗？说下咋用的，哪些场景需要用 1 SET lock_key unique_Val NX PX 10000 lock_key 就是 key 键； unique_Val 是客户端生成的唯一的标识，区分来自不同客户端的锁操作； NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作； PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。 解锁时因为要判断是否是加锁者来解锁，需要完成判断和解锁两个操作。故借助lua脚本完成。\n难点：\n超时时间不好设置。设置短了，任务还未执行完成，其他的线程就可以拿到锁。设置长了，影响性能。为了解决这个问题可以加一个看门狗机制。也就是新增一个守护线程专门监控这种情况。当任务未执行完成，锁快要过期时，续约锁时间。当任务执行完成，但锁还未过期，直接销毁锁即可 主从模式的时候，redis异步复制数据，这将导致分布式锁的不可靠性。如果在主节点获取锁后，其他节点还未同步到锁。主节点宕机了。那么新的主节点还是可以获取到锁。那么如何解决呢。就是redis的redlock红锁 Redis的redlock\n为了保证集群环境下分布式锁的可靠性，Redis 官方已经设计了一个分布式锁算法 Redlock（红锁）。\n它是基于多个 Redis 节点的分布式锁，即使有节点发生了故障，锁变量仍然是存在的，客户端还是可以完成锁操作。官方推荐是至少部署 5 个 Redis 节点，而且都是主节点，它们之间没有任何关系，都是一个个孤立的节点。\nRedlock 算法的基本思路，是让客户端和多个独立的 Redis 节点依次请求申请加锁，如果客户端能够和半数以上的节点成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。\n这样一来，即使有某个 Redis 节点发生故障，因为锁的数据在其他节点上也有保存，所以客户端仍然可以正常地进行锁操作，锁的数据也不会丢失。\nRedlock 算法加锁三个过程：\n第一步是，客户端获取当前时间（t1）。 第二步是，客户端按顺序依次向 N 个 Redis 节点执行加锁操作： 加锁操作使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。 如果某个 Redis 节点发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给「加锁操作」设置一个超时时间（不是对「锁」设置超时时间，而是对「加锁操作」设置超时时间），加锁操作的超时时间需要远远地小于锁的过期时间，一般也就是设置为几十毫秒。 第三步是，一旦客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁，就再次获取当前时间（t2），然后计算计算整个加锁过程的总耗时（t2-t1）。如果 t2-t1 \u0026lt; 锁的过期时间，此时，认为客户端加锁成功，否则认为加锁失败。 可以看到，加锁成功要同时满足两个条件（简述：如果有超过半数的 Redis 节点成功的获取到了锁，并且总耗时没有超过锁的有效时间，那么就是加锁成功）：\n条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 节点上成功获取到了锁； 条件二：客户端从大多数节点获取锁的总耗时（t2-t1）小于锁设置的过期时间。 加锁成功后，客户端需要重新计算这把锁的有效时间，计算的结果是「锁最初设置的过期时间」减去「客户端从大多数节点获取锁的总耗时（t2-t1）」。如果计算的结果已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。\n加锁失败后，客户端向所有 Redis 节点发起释放锁的操作，释放锁的操作和在单节点上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。\n5. 缓存穿透、雪崩的解决方案 缓存穿透：同一时刻大量请求访问不存在的数据。\n缓存空值：可以在缓存中缓存空值来防止压力给到mysql。但是这也不是万能的。缓存空值会浪费我们的空间，如果缓存的空值比较多。还会影响redis的命中率。淘汰了我们正常的热点数据。所以生产环境，我们需要监控一些null空值的数量。避免浪费空间和对正常热数据的影响。我们还可以使用布隆过滤器来处理。 布隆过滤器：查询布隆过滤器，判断数据是否存在。 用法：存储数据时，布隆过滤器会先以hash函数来判断数据的落位bit，以bit位0,1来标示数据是否存在。读取数据时，就可以根据0，1判断是否数据是否存在，而不用去缓存和数据库查找数据。 缺点：1.有hash函数就有hash碰撞，所以就有可能将并不在集合中的元素判断在里面。2.不支持删除元素 解决方案：针对第一点，可以采用多个hash函数来确定bit位，减少误差。多个hash都是1的时候，数据真正存在。(存在疑问？多个hash函数不是更加加大了碰撞的几率嘛。确实是这样的。但是前期数据量比较少的时候，多个hash函数可以显著增加误判率，但是随着越来越多的“1”被占用。碰撞的几率也会越来越大。所以需要去关注布隆过滤器的负载量) 另外布隆过滤器有误判的可能。布隆过滤器判断存在数据存在时，不一定存在(因为hash碰撞占位)。数据不存在时，肯定是不存在，这个是准确的。 缓存雪崩：有大量的key在同一时间过期，或者redis故障\n分散key的失效时间。设置过期时间时，可以加一个时间偏移量 设置key不过期。这个方式比较暴力。但是key并不是真正的会一直存在。redis的多种内存淘汰机制可能会清理掉数据。因此我们需要其他的机制来监控缓存的数据。 可以将缓存更新的任务交由后台线程完成。定时扫描key是否存在。是否需要更新最新数据。业务不负责缓存数据的更新。但是定时扫描的定时需要trade-off，均衡业务的容忍度。 业务请求时发现数据不存在，往mq中写一条消息。交由消费者完成数据的更新和加载。当然执行前可以检测下，数据是否已经存在。 互斥锁。保证不会有大量的请求打到mysql。只有一个请求完成cache aside方式的数据回写。 缓存击穿：热点key数据过期\n互斥锁 任务key不过期 6. 如何保证缓存-db一致性 cache aside方式。选择先更新db，再删除缓存。另外可加消费mysql binlog 的方式或者后台任务。保证更新db和删除操作都执行成功。 7. redis怎么做限流 可以依赖redis单线程和其支持的数据结构完成限流操作。例如：依赖list结构实现令牌桶，固定时间生成token，当令牌token为空时，新来的请求阻塞或者直接返回失败。下面是go-zero框架lua脚本实现令牌桶的方式：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 script = `local rate = tonumber(ARGV[1]) local capacity = tonumber(ARGV[2]) local now = tonumber(ARGV[3]) local requested = tonumber(ARGV[4]) //过期到下次填充前的时间 local fill_time = capacity/rate local ttl = math.floor(fill_time*2) local last_tokens = tonumber(redis.call(\u0026#34;get\u0026#34;, KEYS[1])) if last_tokens == nil then last_tokens = capacity end local last_refreshed = tonumber(redis.call(\u0026#34;get\u0026#34;, KEYS[2])) if last_refreshed == nil then last_refreshed = 0 end //计算上次更新token的时间差，计算出需要补多少token。判断是否满足请求需要的request //计算剩余的token量 local delta = math.max(0, now-last_refreshed) local filled_tokens = math.min(capacity, last_tokens+(delta*rate)) local allowed = filled_tokens \u0026gt;= requested local new_tokens = filled_tokens if allowed then new_tokens = filled_tokens - requested end redis.call(\u0026#34;setex\u0026#34;, KEYS[1], ttl, new_tokens) redis.call(\u0026#34;setex\u0026#34;, KEYS[2], ttl, now) return allowed` 8. redis中server和client通信方式？ 简单文本通信协议 9. redis淘汰策略 noeviction（不淘汰策略）：当内存不足以容纳新写入数据时，新写入操作会报错\u0026quot;OOM（Out of Memory）\u0026quot;。默认情况下，Redis就采用了此种淘汰策略。 allkeys-lru（最近最少使用淘汰策略）：Redis会根据键的最近最少使用时间来淘汰数据。当内存不足以容纳新写入数据时，从所有键中选择最近最少使用的数据淘汰。 volatile-lru（设置过期时间的最近最少使用淘汰策略）：Redis会根据键的最近最少使用时间来淘汰键值对，但只会针对设定了过期时间的键值对进行淘汰。 volatile-ttl（设置过期时间淘汰策略）：Redis会根据键值对的剩余存活时间来淘汰数据。当内存不足以容纳新写入数据时，优先淘汰存活时间较短的数据。此种淘汰策略适用于只关心最近活跃数据的场景。 10. 缓存预热？如何做？ 缓存预热：redis启动前缓存热门数据 nginx+lua将流量请求打到kafka中，让kafka抗高并发的量。然后数据写入storm中，storm完成M份Top N数据的生成。然后聚合生成热门数据存到zk中，新起任务完成读取数据，写入redis。完成预热缓存。 11. Redis 应用题: 设计一个每日日活用户统计功能 需求: 我们的网站每天用户访问之后，UserID 将会被收集到我们的 Redis 数据存储（需要你思考如何存储），我们的后台需要支持一个功能: 1.API - 支持查询 某一天访问的用户 \u0026amp; 前一天并没有访问我们的网站 Request : {\u0026quot;date\u0026quot;: \u0026quot;20230216\u0026quot;} Response: {\u0026quot;user_ids\u0026quot;: [1,3,4]} 2.API 拓展 - 在支持(1) 的基础上，支持根据指定批量日期范围查询这个数据 Request : {\u0026quot;dates\u0026quot;: [\u0026quot;20230216\u0026quot;, \u0026quot;20230217\u0026quot;]} Response: {\u0026quot;list\u0026quot;: [{\u0026quot;date\u0026quot;: \u0026quot;20230216\u0026quot;, \u0026quot;user_ids\u0026quot;: [1,3,4]}, {\u0026quot;date\u0026quot;: \u0026quot;20230216\u0026quot;, \u0026quot;user_ids\u0026quot;: [2,9]}]} 数据量级: Level0: 每日访问用户数在 1K 以内 Level1: 每日访问用户数在 10K 以内 考察点1: redis 数据存储如何设计，应该使用什么数据结构，怎么记录和实现功能 答案： 1.按照天的维度将用户放到set结构中(key是日期，value是UserID)，针对查询需求，可以将两天的set做diff操作，生成到一个new set中。 2.如果要支持指定批量日期范围查询，首先需要明确最早日期，日期数量是否有限制？qps是多少(每条执行多少请求和计算量有关)，如果不高的话，可以实时diff计算， 一般情况下，latency会比较长。 latency过长的解决方案： 1.缓存：new set已经放入到了redis中，如果请求中的日期过多，影响了latency的性能，那么可以使用进程内缓存，此时就要考虑缓存的大小如何设计？(设计的标准应该是满足当前qps，多少数据从redis拿，多少从进程缓存拿，可以满足qps需求)，考虑设计缓存淘汰方式(LRU、LFU or 其他？，这里选择LFU，淘汰使用频率最少的比较合理)。 2.升级机器配置，升级cpu配置，升级redis配置。使得机器cpu不是瓶颈(内存应该不是瓶颈，不需要考虑)，升级redis的cpu配置，主从或者分片。 数量级问题需要注意的点：1k以内的set，userid是int类型的话8byte，一个日期的key存储占8k(set是hash结构和intset，那就按照正常的数组大小评估，就是8k)，不算是大key(大key的标准和结构中的数量和元素的大小相关)，那么diff完成的也不是大key。这个时候就还是考虑latency的问题了，按照上面方案解决就行。 如果真的数量级到了大key，那么一般方案也就是拆分成多个key，此时内存友好，但是cpu就不友好了。解决的话，可以升级配置。或者redis集群部署，分散压力。 12. Redis的主从同步流程 全量同步： 触发条件：首次同步，master节点进程id发生变化，master缓存中没有slave当前同步的offsetID(滞后太多) 同步过程： slave向master发起链接建立。 slave向master发起sync命令，请求复制数据 master收到PSYNC命令后，会在后台进行数据持久化； 通过bgsave生成最新的rdb快照文件 bgsave期间，将客户端发送的命令（会修改数据集的）缓存到内存中； 持久化完毕后，master将这份RDB数据发送给slave； slave会把接收到的数据进行持久化生成RDB，然后再加载到内存中。 master继续将之前缓存在内存中的命令通过socket发送给slave。 增量，断点续传同步： 同步过程： 断开重连后，slave向master发送psync命令同步数据 master会在内存中维护命令队列repl backlong buff，并且master拥有各slave的数据下标offset，slave有master进程id 重连后，从下标开始接受master缓冲队列的命令，完成同步 如果master进程节点id变化，或者从节点数据下标offset太久，已经不在master的缓存队列里，则会进行一次全量数据复制 注意点：\n主从复制时，如果有过多的从节点，为了缓解主从复制风暴，多个从节点同事复制主节点导致主节点压力过大。可以让部分从节点作为假主，关联其他从库。也就是主-从(假主)-从 RDB用于全量复制，全量复制完后，后续的执行指令，master会将repl backlong buff中缓存的指令通过socket发送给从库。 主从复制的时候，针对slave进行全量数据同步，slave在加载master的RDB文件前，会运行flushall来清理自己当前的数据。slave-lazy-flush参数设置决定是否采用异步flush的机制。异步flush清空从节点本地数据库，可减少全量同步耗时，从而减少主库因输出缓冲区爆炸引起的内存使用增长。 13. Redis多线程网络IO模型(下面的内容都来自潘少的blog) redis为什么这么快？ C语言支持 内存型数据库 网络IO多路复用 单线程模型，避免多线程频繁上下文切换，额外的同步机制 高性能的数据结构设计 redis为什么选择单线程 避免过多的上下文切换开销 多线程调度必然会有上下文切换的开销，具体的开销在于程序计数器，堆栈指针和程序状态等一系列寄存器置换，程序堆栈重置等，还会影响cpu cache，TLB快表的淘汰置换。单一进程的多线程切换还好一些，因为多个线程共享进程地址空间。对比跨进程(多进程)的调度，那么开销就会很大，需要切换掉整个进程的地址空间。 避免同步机制的开销 多线程模型下，势必会有一些同步机制，用来保证数据的状态。简单来说就是需要加锁了。因为redis有丰富的数据类型，此时锁的粒度又会有很多取舍。这样不仅会有锁的开销，程序的复杂度也会极具增长。与redis简单设计的理念不符 redis多线程发展历程 redis 4.0的时候引入了多线程处理异步任务。这些任务包括数据清理，数据备份bgsave等。主要的目的避免一些阻塞的命令，影响单线程执行模型。比如新增了UNLINK、FLUSHALL ASYNC等。但网络IO处理还是单线程的。 redis 6.0。 Reactor 单进程 / 线程，不用考虑进程间通信以及数据同步的问题，因此实现起来比较简单，这种方案的缺陷在于无法充分利用多核 CPU，而且处理业务逻辑的时间不能太长，否则会延迟响应，所以不适用于计算机密集型的场景，适用于业务处理快速的场景，比如 Redis（6.0之前 ） 采用的是单 Reactor 单进程的方案。 发展到这个阶段，多线程来提升整体性能已经是必定的路了。因为redis的单线程模型会导致系统消耗很多cpu在网络IO上，从而降低了吞吐量，那么为了解决这个问题，只能1.优化网络IO 2.提高机器内存读写速度。显然完成条件1是首先考虑的。优化1是手段有1.零拷贝 2.利用多核优势。零拷贝无法应对redis这一类复杂的网络IO场景。所以顺理成章的我们需要利用多核优势了。 6.0的时候，增加多线程来处理网络IO的读写，就是多线程去处理IO读写数据，但执行操作还是单线程的。官方的数据显示增加多线程后，性能提升一倍。 cpu亲和性 redis6.0后，多线程执行很多异步任务，并发度已经有很大提升，而且redis是对吞吐量和延迟很敏感的系统。故此需要考虑cpu切换时的性能损耗，为了避免这个情况，redis启动的时候设置了cpu亲和性，也就是绑核，将某些线程/进程绑定到固定的cpu上，其他任务就不会占用这些任务的cpu时间片，能更高效率的工作，也能极大限度借助cpu cache提升性能。 free lock的设计 引入多线程，不可避免的会有共享资源竞争，就会引出锁来保护共享资源。但是redis通过原子操作和时空交错访问来实现无锁的多线程模型。记住redis6.0的源码中，IO读写多线程只完成读写数据的操作。 下面是多线程读的流程。 当有client就绪时，就会将client放入队列clients_pending_read 主线程遍历待读取的client队列，通过 RR 策略把所有任务分配给 I/O 线程和主线程去读取和解析客户端命令。 忙轮询等待所有 I/O 线程完成任务。 完成任务后，再遍历 clients_pending_read，执行所有 client 的命令。 执行完成后，需要回复的内容已经写入每个client的buf中了。然后将client放入clients_pending_write队列中，等待调度，将响应内容返回给客户端。 下面是多线程写的流程： 检查当前任务负载，如果当前的任务数量不足以用多线程模式处理的话，则休眠 I/O 线程并且直接同步将响应数据回写到客户端。 唤醒正在休眠的 I/O 线程（如果有的话）。 遍历待写出的 client 队列 clients_pending_write，通过 RR 策略把所有任务分配给 I/O 线程和主线程去将响应数据写回到客户端。 忙轮询等待所有 I/O 线程完成任务。 最后再遍历 clients_pending_write，为那些还残留有响应数据的 client 注册命令回复处理器 sendReplyToClient，等待客户端可写之后在事件循环中继续回写残余的响应数据。 为什么是lock free呢？ 主线程和 I/O 线程之间共享的变量有三个：io_threads_pending 计数器、io_threads_op I/O 标识符和 io_threads_list 线程本地任务队列。io_threads_pending 是原子变量，不需要加锁保护，io_threads_op 和 io_threads_list 这两个变量则是通过控制主线程和 I/O 线程交错访问来规避共享数据竞争问题：I/O 线程启动之后会通过忙轮询和锁休眠等待主线程的信号，在这之前它不会去访问自己的本地任务队列 io_threads_list[id]，而主线程会在分配完所有任务到各个 I/O 线程的本地队列之后才去唤醒 I/O 线程开始工作，并且主线程之后在 I/O 线程运行期间只会访问自己的本地任务队列 io_threads_list[0] 而不会再去访问 I/O 线程的本地队列，这也就保证了主线程永远会在 I/O 线程之前访问 io_threads_list 并且之后不再访问，保证了交错访问。io_threads_op 同理，主线程会在唤醒 I/O 线程之前先设置好 io_threads_op 的值，并且在 I/O 线程运行期间不会再去访问这个变量。 ","permalink":"https://www.youngergo.cn/en/posts/tech/redis-interview/","summary":"1. 持久化怎么实现的 aof：append only file。持续写文件到buffer ring中，然后根据参数完成fsync操作。aof文件会有重写操作","title":"Redis Interview"},{"content":"go netpooler-网络io的实现细节: 首先，client连接server的时候，listener会通过accept函数接受到一个新的connection，每一个connection会启动一个goroutine来处理读写， accept会将该connection的fd连带goroutine信息封装注册到epoll的监听列表中，当G调用conn.read或者conn.write等需要阻塞等待的函数时， 会调用gopark将当前G挂起休眠，让P执行下一个G。往后会由Go scheduler在循环调度runtime.schedule()函数或者sysmon监控线程中调用runtime.netpoll 以获取已就绪的G列表并通过inhectglist把G放入全局调度队列或者当前P队列中去执行。\n当IO事件发生之后，netpoller是通过runtime.netpoll函数唤醒那些I/O wait状态的G。它的主要逻辑是：\n根据调用方的入参delay，设置对应的epollwait的timeout值 调用epollwait等待发生了可读可写的事件fd 循环遍历epollwait返回的事件列表，处理对应的读写事件类型，组装可运行的G链表并返回 runtime.netpoll在很多场景下都会被调用。它会调用epoll_wait系统函数从epoll的eventpoll.rdllist就绪双向列表返回，从而得到了就绪的socket fd列表， 并取出最初调用epoll_ctl时保存的上下文信息恢复G。 所以执行完netpoll之后，会返回一个就绪的G链表（包含对应的就绪fd）接下来将就绪的G通过injectglist加入到全局调度队列或者P的本地队列去执行。\ngo内存逃逸 1、堆上动态分配内存比栈上静态分配内存，开销大很多。\n2、变量分配在栈上需要能在编译期确定它的作用域，否则会分配到堆上。\n3、Go编译器会在编译期对考察变量的作用域，并作一系列检查，如果它的作用域在运行期间对编译器一直是可知的，那么就会分配到栈上。简单来说，编译器会根据变量是否被外部引用来决定是否逃逸。\n4、对于Go程序员来说，编译器的这些逃逸分析规则不需要掌握，我们只需通过go build -gcflags \u0026lsquo;-m’命令来观察变量逃逸情况就行了。\n5、不要盲目使用变量的指针作为函数参数，虽然它会减少复制操作。但其实当参数为变量自身的时候，复制是在栈上完成的操作，开销远比变量逃逸后动态地在堆上分配内存少的多。\n6、逃逸分析在编译阶段完成的。\ngo空struct有什么用 空结构体也是结构体，只是 size 为 0 的类型而已； 所有的空结构体都有一个共同的地址：zerobase 的地址； 空结构体可以作为 receiver ，receiver 是空结构体作为值的时候，编译器其实直接忽略了第一个参数的传递，编译器在编译期间就能确认生成对应的代码； map 和 struct{} 结合使用常常用来节省一点点内存，使用的场景一般用来判断 key 存在于 map； chan 和 struct{} 结合使用是一般用于信号同步的场景，用意并不是节省内存，而是我们真的并不关心 chan 元素的值； slice 和 struct{} 结合好像真的没啥用。。。 new和malloc的区别 来源：https://www.cnblogs.com/33debug/p/12068699.html\n原理 make 在编译期的类型检查阶段，Go语言其实就将代表 make 关键字的 OMAKE 节点根据参数类型的不同转换成了 OMAKESLICE、OMAKEMAP 和 OMAKECHAN 三种不同类型的节点， 这些节点最终也会调用不同的运行时函数来初始化数据结构。 new 内置函数 new 会在编译期的 SSA 代码生成阶段经过 callnew 函数的处理，如果请求创建的类型大小是 0，那么就会返回一个表示空指针的 zerobase 变量， 在遇到其他情况时会将关键字转换成 newobject。原理如上所述。 主要区别如下：\nmake 只能用来分配及初始化类型为 slice、map、chan 的数据。new 可以分配任意类型的数据； new 分配返回的是指针，即类型 *Type。make 返回引用，即 Type； new 分配的空间被清零。make 分配空间后，会进行初始化； 总结 判定对象大小： 若是微小对象： 从 mcache 的 alloc 找到对应 classsize 的 mspan； 当前mspan有足够的空间时，分配并修改mspan的相关属性（nextFreeFast函数中实现）； 若当前mspan没有足够的空间，从 mcentral 重新获取一块对应 classsize的 mspan，替换原先的mspan，然后分配并修改mspan的相关属性； 若 mcentral 没有足够的对应的classsize的span，则去向mheap申请； 若对应classsize的span没有了，则找一个相近的classsize的span，切割并分配； 若找不到相近的classsize的span，则去向系统申请，并补充到mheap中； 若是小对象，内存分配逻辑大致同小对象： 查表以确定需要分配内存的对象的 sizeclass，找到 对应classsize的 mspan； mspan有足够的空间时，分配并修改mspan的相关属性（nextFreeFast函数中实现）； 若当前mspan没有足够的空间，从 mcentral重新获取一块对应 classsize的 mspan，替换原先的mspan，然后分配并修改mspan的相关属性； 若mcentral没有足够的对应的classsize的span，则去向mheap申请； 若对应classsize的span没有了，则找一个相近的classsize的span，切割并分配 若找不到相近的classsize的span，则去向系统申请，并补充到mheap中 若是大对象，直接从mheap进行分配 若对应classsize的span没有了，则找一个相近的classsize的span，切割并分配； 若找不到相近的classsize的span，则去向系统申请，并补充到mheap中； Mysql exist in 执行效率 1、exists是对外表做loop循环，每次loop循环再对内表（子查询）进行查询，那么因为对内表的查询使用的索引（内表效率高，故可用大表），而外表有多大都需要遍历，不可避免（尽量用小表），故内表大的使用exists，可加快效率，包括内表是分组筛选后的结果比外表小的情况；\n2、in是把外表和内表做join连接，先查询内表，再把内表结果与外表匹配，对外表使用索引（外表效率高，可用大表），而内表多大都需要查询，不可避免，故外表大的使用in，可加快效率。\n3、如果用not in ，则是内外表都全表扫描，无索引，效率低，可考虑使用not exists，也可使用A left join B on A.id=B.id where B.id is null 进行优化。\n4、结论： 1.当内外表的数据量差不多时，in和exist的效率是差不多的 2.当外表大，内表小时，in执行效率高 3.外表小，内表大时，exist的执行效率高 4.mysql高性能编程里面讲查询优化会讲in转化为exist执行。\nTCP的time_wait状态的知识点 time wait存在的意义有两个： a. 作为全双工连接的tcp，需要保证连接关闭的准确状态。当主动关闭的一端接收到FIN报文时，主动端发出ACK后会处于time wait状态。此时会保持状态2MSL时间(RFC默认时间是60s)， 如果对方未收到最后的ack时，会重发FIN包，此时就可以重新处理这个包，保证连接的正常结束，双方都不要有丢包的问题。 b. tcp连接有keep-alive的特性，避免重新建立，关闭连接带来的性能开销。如果网络包因为DNS等问题，在网络中丢失方向，TCP发送端就会因为确认超时，重发这个包，这些包最终也会被送到目的地。 此时如果重新使用了当前的四元组，建立了新链接。如果不等待的话，可能会收到这些\u0026quot;迷途知返\u0026quot;包。为了避免这个情况，TCP不允许处于TIME_WAIT状态的连接启动一个新连接。等待2MSL时间，可以确保当成功建立一个新连接时， 来自旧链接的数据包在网络中已经消逝。 time_wait状态带来的影响 在高并发的短连接场景下，time_wait状态会大量占用port信息(16bit,65535个)，影响新连接的建立。具体的原因是：数据请求返回时间+业务处理时间\u0026lt;time_wait时间，此时机器的性能 主要集中在等待资源释放上， 如何尽量处理TIMEWAIT过多? 编辑内核文件/etc/sysctl.conf，加入以下内容：\n1 2 3 4 net.ipv4.tcp_syncookies = 1 表示开启SYN Cookies。当出现SYN等待队列溢出时，启用cookies来处理，可防范少量SYN攻击，默认为0，表示关闭； net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭； net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。 net.ipv4.tcp_fin_timeout 修改系默认的 TIMEOUT 时间 然后执行 /sbin/sysctl -p 让参数生效.\n/etc/sysctl.conf是一个允许改变正在运行中的Linux系统的接口，它包含一些TCP/IP堆栈和虚拟内存系统的高级选项，修改内核参数永久生效。 简单来说，就是打开系统的TIME_WAIT重用和快速回收。\n如果以上配置调优后性能还不理想，可继续修改一下配置： 复制代码\n1 2 3 4 5 6 7 8 9 10 vi /etc/sysctl.conf net.ipv4.tcp_keepalive_time = 1200 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。 net.ipv4.ip_local_port_range = 1024 65000 #表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。 net.ipv4.tcp_max_syn_backlog = 8192 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_tw_buckets = 5000 #表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。 默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少TIME_WAIT套接字数量 go slice的扩容 GO1.17版本及之前 当新切片需要的容量cap大于两倍扩容的容量，则直接按照新切片需要的容量扩容； 当原 slice 容量 \u0026lt; 1024 的时候，新 slice 容量变成原来的 2 倍； 当原 slice 容量 \u0026gt; 1024，进入一个循环，每次容量变成原来的1.25倍,直到大于期望容量。\nGO1.18之后 当新切片需要的容量cap大于两倍扩容的容量，则直接按照新切片需要的容量扩容； threshold=256 当原 slice 容量 \u0026lt; threshold 的时候，新 slice 容量变成原来的 2 倍； 当原 slice 容量 \u0026gt; threshold，进入一个循环，每次容量增加（旧容量+3*threshold）/4。\n17中的扩容存在问题，当容量刚超过1024时，其新增容量回比之前有回落。18中新的方案有两个点，一个是1024\u0026ndash;》256，另一个是扩容公式（旧容量+3*threshold）/4， 从commit的实验数据来看，当容量越来越大时，容量的增长比率会接近1.25倍。这次的更改让扩容的整体增长曲线更加平滑。\ngo的GC go刚开始的标记清除策略是需要STW维护对象的状态的，演变到三色标记时，主要是想降低STW对用户进程的影响，希望用户进程和标记可以并发执行。但是这样的话 对象就有可能被用户进程修改占用。比如已经标记的A对象(黑色)，指向了白色对象。清扫阶段会清理掉白色，A的对象引用就丢失了，称为悬挂指针。针对这种情况， 在标记阶段，对象指针发生变更时，需要满足下面情况，提出强弱三色不变性： 1.黑色对象不能指向白色对象，只能指向黑色和灰色 2.黑色对象指向的白色对象，必须有一条路径，从灰色对象出发经历多个白色对象后，可达这个白色对象 遵循上述两个不变性中的任意一个，我们都能保证垃圾收集算法的正确性，而屏障技术就是在并发或者增量标记过程中保证三色不变性的重要技术。 所谓的屏障技术，简单来说，就是控制对内存操作的顺序性，屏障之前的代码比之后的代码先执行，这主要是防止多核cpu架构下，对内存访问的优化操作。在GC中具体的做法 就是，在gc和用户程序并发的情况下，当用户程序对对象进行增删改时，执行屏障逻辑，改变操作的对象的着色标记。\nGolang使用的是三色标记法方案，并且支持并行GC，即用户代码可以和GC代码同时运行。具体来讲，Golang GC分为几个阶段:\nMark阶段该阶段又分为两个部分： Mark Prepare：初始化GC任务，包括开启写屏障(write barrier)和辅助GC(mutator assist)，统计root对象的任务数量等，这个过程需要STW。 GC Drains: 扫描所有root对象，包括全局指针和goroutine(G)栈上的指针（扫描对应G栈时需停止该G)，将其加入标记队列(灰色队列)，并循环处理灰色队列的对象，直到灰色队列为空。该过程后台并行执行。 Mark Termination阶段：该阶段主要是完成标记工作，重新扫描(re-scan)全局指针和栈。因为Mark和用户程序是并行的，所以在Mark过程中可能会有新的对象分配和指针赋值，这个时候就需要通过写屏障（write barrier）记录下来，re-scan再检查一下，这个过程也是会STW的。 Sweep: 按照标记结果回收所有的白色对象，该过程后台并行执行。 Sweep Termination: 对未清扫的span进行清扫, 只有上一轮的GC的清扫工作完成才可以开始新一轮的GC。 总结一下，Golang的GC过程有两次STW:第一次STW会准备根对象的扫描, 启动写屏障(Write Barrier)和辅助GC(mutator assist).第二次STW会rescan新标记的灰色对象， 禁用写屏障(Write Barrier)和辅助GC(mutator assist).\n在 Go 语言 v1.7 版本之前，运行时会使用 Dijkstra 插入写屏障保证强三色不变性，但是运行时并没有在所有的垃圾收集根对象上开启插入写屏障。因为应用程序可能包含成百上千的 Goroutine， 而垃圾收集的根对象一般包括全局变量和栈对象，如果运行时需要在几百个 Goroutine 的栈上都开启写屏障，会带来巨大的额外开销，所以 Go 团队在实现上选择了在标记阶段完成时暂停程序、将所有栈对象标记为灰色并重新扫描， 在活跃 Goroutine 非常多的程序中，重新扫描的过程需要占用 10 ~ 100ms 的时间。\nGo 语言在 v1.8 组合 Dijkstra 插入写屏障和 Yuasa 删除写屏障构成了如下所示的混合写屏障，该写屏障会将被覆盖的对象标记成灰色并在当前栈没有扫描时将新对象也标记成灰色：\n1 2 3 4 5 writePointer(slot, ptr): shade(*slot) if current stack is grey: shade(ptr) *slot = ptr 为了移除栈的重扫描过程，除了引入混合写屏障之外，在垃圾收集的标记阶段，我们还需要将创建的所有新对象都标记成黑色，防止新分配的栈内存和堆内存中的对象被错误地回收， 因为栈内存在标记阶段最终都会变为黑色，所以不再需要重新扫描栈空间。\n总结一下：1.7版本之前没有混合写屏障时，使用插入写屏障来保证三色不变性，但是为了性能，并不会在所有的G上开启写屏障。所以会在标记阶段完成后，开启STW，栈对象全部置灰， 重新扫描re-scan。而Yuasa 删除写屏障，标记结束不需要STW，但是回收精度低，会有已删除的引用，在下次时才清理。为了优化stw的这个过程和提高效率， 1.8引入混合写屏障，大大缩减了stw的时间，具体如下：\nGC 开始将栈上的对象全部扫描并标记为黑色；\nGC 期间，任何在栈上创建的新对象，均为黑色(不用re-scan栈)；\n被删除的堆对象标记为灰色；\n被添加的堆对象标记为灰色；\n阶段\t说明\t赋值器状态 SweepTermination\t清扫终止阶段，为下一阶段的并发标记做准备工作，启动写屏障\tSTW Mark\t扫描标记阶段，与赋值器并发执行，写屏障开启\t并发 MarkTermination\t标记终止阶段，保证一个周期内标记任务完成，停止写屏障\tSTW GCoff\t内存清扫阶段，将需要回收的内存归还到堆中，写屏障关闭\t并发 GCoff\t内存归还阶段，将需要回收的内存归还给操作系统，写屏障关闭\t并发\n知识点： 删除写屏障：也叫做基于其实快照的解决方案（snapshot-at-the-begining）。顾名思义，就是在开始 gc 之前，必须 STW ，对整个根做一次起始快照。 当赋值器（业务线程）从灰色或者白色对象中删除白色指针时候，写屏障会捕捉这一行为，将这一行为通知给回收器。这样，基于起始快照的解决方案保守地将其目标对象当作存活的对象， 这样就绝对不会有被误回收的对象，但是有扫描工作量浮动放大的风险。术语叫做追踪波面的回退。\n删除写屏障（基于起始快照的写屏障）有一个前提条件，就是起始的时候，把整个根部扫描一遍，让所有的可达对象全都在灰色保护下（根黑，下一级在堆上的全灰）， 之后利用删除写屏障捕捉内存写操作，确保弱三色不变式不被破坏，就可以保证垃圾回收的正确性。\n","permalink":"https://www.youngergo.cn/en/posts/tech/go-%E7%9F%A5%E8%AF%86%E7%82%B9/","summary":"go netpooler-网络io的实现细节: 首先，client连接server的时候，listener会通过accept函数接受到一个新的co","title":"Go 知识点"},{"content":"简介 众所周知，golang的goroutine是go并发编程的基础知识，我们只需要使用简单go func()就可以开启一个go协程，然后调度器会管理go协程分配相应的P和M，执行相应的业务逻辑，而且开启一个goroutine代价很小，只会占用2k的栈空间。但是协程也不是随意想开多少就多少，过多的goroutine不仅会影响调度器的执行时间，也会造成内存的快速增长。\n今天给大家分享一个github上有1w star的协程池库ants。\u0026ldquo;ants是一个高性能的 goroutine 池，实现了对大规模 goroutine 的调度管理、goroutine 复用，允许使用者在开发并发程序的时候限制 goroutine 数量，复用资源，达到更高效执行任务的效果。从benchmark的结果来看，使用ants的吞吐性能相较于原生 goroutine 可以保持在 2-6 倍的性能压制，而内存消耗则可以达到 10-20 倍的节省优势\u0026rdquo;(来自仓库描述)，具体的细节可以看git仓库。\n设计理念 ants的设计理念是使用N个work goroutine 完成M个task的执行。控制goroutine的数量。就像仓库中的Activity Diagrams描述的那样。但是这样的设计也会有一些弊端，我们后面再聊(图片来自原github仓库)。 目录结构 ants的代码并不多，除了test外总共也只有十多个文件，而且代码行数也不多，下面是比较重要的几个文件：\nants.go ants.go 封装了外部调用的能力，它会初始化一个无 goroutine 限制的defaultAntsPool对象，提供了任务提交，获取运行协程数量，获取容量等能力。 pool.go pool.go 实现协程池需要的能力。协程池的定义，任务提交，分配协程等逻辑实现，都在这文件中，ants中的defaultAntsPool调用这个文件中的实现。 pool_func.go pool_func.go 和pool.go做的事情差不多，区别在于pool.go可以执行不同的协程任务，pool_func.go 执行的是相同的任务，但是可以传不同的参数。使用场景不同。 work.go work.go 完成具体任务的执行goWork，实例化了work_queue.go中worker interface的实现。和pool.go关联。 work_func.go work_func.go 和work.go的做了同样的事情，它和pool_func.go关联。 worker_queue.go worker_queue.go worker_queue是管理worker执行的队列，定义了worker 和worker queue需要实现的能力。 worker_loop_queue.go worker_loop_queue.go 用队列的形式实现对woker队列的管理。 worker_stack.go worker_stack.go 以栈的形式实现worker队列管理 options.go options.go ants对象的一些配置特性 这些就是ants的主要文件，他们之间的关系很简单，workerQueue interface定义了worker队列的能力，有ring queue和stack两种实现。woker interface定义了work的能力，有work和work_func两种实现。协程池也就有对应pool和pool_func两种实现。\n源码解析 ants的代码并不复杂，但是有很多细节，它实现了两种能力的协程池，pool是可以执行不同任务的协程池，poolWithFunc只能执行相同的任务，但是参数可以不同。对应此两种能力，所以有了goWorker和goWokerWithFunc两种worker。我们先介绍pool相关源码(pool关联work.go中的goWorker)，大家可以对照阅读poolWithFunc的代码。首先我们先从example入手：(一般开源的代码都会有example文件，我们学习之前可以先从这些示例入手，然后针对感兴趣的模块逐层深入，这样效率会高点)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 //https://github.com/panjf2000/ants/blob/dev/examples/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/panjf2000/ants/v2\u0026#34; ) var sum int32 func myFunc(i interface{}) { n := i.(int32) atomic.AddInt32(\u0026amp;sum, n) fmt.Printf(\u0026#34;run with %d\\n\u0026#34;, n) } func demoFunc() { time.Sleep(10 * time.Millisecond) fmt.Println(\u0026#34;Hello World!\u0026#34;) } func main() { defer ants.Release() runTimes := 1000 // Use the common pool. var wg sync.WaitGroup syncCalculateSum := func() { demoFunc() wg.Done() } for i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = ants.Submit(syncCalculateSum) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, ants.Running()) fmt.Printf(\u0026#34;finish all tasks.\\n\u0026#34;) // Use the pool with a method, // set 10 to the capacity of goroutine pool and 1 second for expired duration. p, _ := ants.NewPoolWithFunc(10, func(i interface{}) { myFunc(i) wg.Done() }) defer p.Release() // Submit tasks one by one. for i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = p.Invoke(int32(i)) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, p.Running()) fmt.Printf(\u0026#34;finish all tasks, result is %d\\n\u0026#34;, sum) if sum != 499500 { panic(\u0026#34;the final result is wrong!!!\u0026#34;) } } 测试代码介绍了pool和poolWithFunc的使用。syncCalculateSum函数就是我们需要执行的任务，ants.Submit(syncCalculateSum)将任务提交到pool中，然后会分配给work去执行。那么我们去看看Submit具体实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 //首先先了解下goWorker的定义 //https://github.com/panjf2000/ants/blob/dev/work.go type goWorker struct { // 协程池对象，其实多个goWorker共享的是一个pool对象，在创建worker是传的同一个p，下面代码会讲。 pool *Pool // 接受任务的有缓冲的chan task chan func() // 上次使用时间，主要是清理worker时使用 lastUsed time.Time } //https://github.com/panjf2000/ants/blob/dev/pool.go type Pool struct { // 协程池的容量，负数代表无限制 capacity int32 //当前在运行的worker数量 running int32 //保护从队列中读写worker lock sync.Locker // workers队列，这个队列有stack和ring queue两种实现 workers workerQueue //协程池状态 state int32 //自旋锁，等待获取空闲的worker cond *sync.Cond // workerCache 使用对象池，加快了在 function:retrieveWorker 中获取可用 Worker 的速度。 workerCache sync.Pool // 阻塞在cond上的任务数量 waiting int32 //清理等待时间过长的worker purgeDone int32 stopPurge context.CancelFunc //更新worker的使用时间，也是用于清理 ticktockDone int32 stopTicktock context.CancelFunc //使用时间 now atomic.Value //配置信息 options *Options } //https://github.com/panjf2000/ants/blob/dev/pool.go func (p *Pool) Submit(task func()) error { //判断协程池是否关闭 if p.IsClosed() { return ErrPoolClosed } //检索出一个可用的work，然后将任务交给work去执行 if w := p.retrieveWorker(); w != nil { w.inputFunc(task) return nil } return ErrPoolOverload //https://github.com/panjf2000/ants/blob/dev/pool.go func (p *Pool) retrieveWorker() (w worker) { //从缓存的对象池中获取一个worker，启动worker接受任务，这里可能会new一个，也可能复用 spawnWorker := func() { w = p.workerCache.Get().(*goWorker) w.run() } //加锁保护worker的获取 p.lock.Lock() w = p.workers.detach() if w != nil { //如果成功获取一个worker，直接返回 p.lock.Unlock() } else if capacity := p.Cap(); capacity == -1 || capacity \u0026gt; p.Running() { //如果初始化是未限制worker的数量，或者当前在运行的worker数量还没到达限制。就调用spawnWorker获取一个 p.lock.Unlock() spawnWorker() } else { //否则证明worker已经使用完了。如果用户设置了不阻塞，就直接返回nil，返回错误给用户 if p.options.Nonblocking { p.lock.Unlock() return } retry: //等待空闲的worker释放，如果等待的任务超出限制，直接返回 if p.options.MaxBlockingTasks != 0 \u0026amp;\u0026amp; p.Waiting() \u0026gt;= p.options.MaxBlockingTasks { p.lock.Unlock() return } //cond是实现的指数退避的自旋锁。自旋等待worker释放 p.addWaiting(1) p.cond.Wait() p.addWaiting(-1) //有worker可用或者是pool关闭了，关闭直接返回 if p.IsClosed() { p.lock.Unlock() return } //work可能被其他等待的任务获取了，如果还未到达容量上限，就从对象池拿一个 //否则重试继续获取 if w = p.workers.detach(); w == nil { if p.Free() \u0026gt; 0 { p.lock.Unlock() spawnWorker() return } goto retry } p.lock.Unlock() } //成功获取，返回 return } //https://github.com/panjf2000/ants/blob/dev/worker.go func (w *goWorker) inputFunc(fn func()) { //将任务丢给chan，会有worker去执行 w.task \u0026lt;- fn } func (w *goWorker) run() { w.pool.addRunning(1) go func() { //PanicHandler可以设置回调函数，gowoker panic的时候可以做些措施 defer func() { w.pool.addRunning(-1) w.pool.workerCache.Put(w) if p := recover(); p != nil { if ph := w.pool.options.PanicHandler; ph != nil { ph(p) } else { w.pool.options.Logger.Printf(\u0026#34;worker exits from panic: %v\\n%s\\n\u0026#34;, p, debug.Stack()) } } // Call Signal() here in case there are goroutines waiting for available workers. w.pool.cond.Signal() }() //每一个goworker都会启动一个协程从chan中读取任务执行，执行完成后将尝试将worker放到pool的队列中。 //如果放入失败，证明协程池中可运行协程数已满 or 当前协程池已经关闭，退出当前goroutine，将goworker放到缓存中。 //如果成功的话，证明当前协程池没满也没关闭，那这个协程就会阻塞在task chan上读取任务，此时这个协程是没有退出的 //goworker对象进入worker队列，等待新任务调度，有新的任务来时，等调度到goworker，然后将任务写入task chan for f := range w.task { if f == nil { return } f() if ok := w.pool.revertWorker(w); !ok { return } } }() } //尝试将goworker放到woker队列中管理 func (p *Pool) revertWorker(worker *goWorker) bool { //如果池子中运行的worker已经大于最大容量，或者池子已经关闭。返回放入失败 if capacity := p.Cap(); (capacity \u0026gt; 0 \u0026amp;\u0026amp; p.Running() \u0026gt; capacity) || p.IsClosed() { //这里的broadcast我查了资料，没有找到具体的原因，为什么放入失败需要唤醒所有等待的协程。 //我个人的理解是，唤醒retrieveWorker中所有阻塞在p.cond.wait()的协程，函数中会在retry时重新执行 //等待执行的任务是否超过用户设置的MaxBlockingTasks的逻辑。 p.cond.Broadcast() return false } //更新此worker使用时间 worker.lastUsed = p.nowTime() p.lock.Lock() // 这里为了避免内存泄漏，二次检查池的状态。具体泄漏场景大家可以看看下面的issue // Issue: https://github.com/panjf2000/ants/issues/113 if p.IsClosed() { p.lock.Unlock() return false } //调用worker queue的方法，将worker放入队列管理 if err := p.workers.insert(worker); err != nil { p.lock.Unlock() return false } //通知所有阻塞在\u0026#39;retrieveWorker()\u0026#39; 方法的协程，有新的worker可用 p.cond.Signal() p.lock.Unlock() return true } 整个流程比较简单，调用submit函数提交自定义的任务给协程池，submit调用retrieveWorker获取一个可用的worker，然后在调用inputFunc函数将任务交给这个worker去执行。有几个点需要注意下，workerCache使用了对象池，缓存了goworker对象，从池中Get时，可能会new一个woker(程序刚启动时，还未到达cap限制)，也可能会读取使用过的gowoker。还有就是使用了自旋锁sync.Cond等待可用的worker。还有个比较重要的是goworker的run函数，每一个goworker都会启动一个协程从chan中读取任务来执行。每次执行一个任务，都会尝试将此worker重新放入pool的队列中，等待再次调度。如果放入失败，就会放到workerCache对象池，最大程度复用对象。\n上面就是整个worker调用和分配的的流程，整体逻辑不是很复杂，但是有很多细节需要注意。下面顺着上面的逻辑，我们介绍下worker_queue\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 //https://github.com/panjf2000/ants/blob/dev/worker_queue.go type worker interface { run() finish() lastUsedTime() time.Time inputFunc(func()) inputParam(interface{}) } type workerQueue interface { len() int isEmpty() bool insert(worker) error detach() worker refresh(duration time.Duration) []worker reset() } type queueType int const ( queueTypeStack queueType = 1 \u0026lt;\u0026lt; iota queueTypeLoopQueue ) func newWorkerQueue(qType queueType, size int) workerQueue { switch qType { case queueTypeStack: return newWorkerStack(size) case queueTypeLoopQueue: return newWorkerLoopQueue(size) default: return newWorkerStack(size) } Pool对象中有workers workerQueue对象，workerQueue管理所有的worker，根据newWorkerQueue方法，我们可以发现workerQueue有两种实现，loopQueue实现了ring queue，底层是slice数组加一些头尾标识来实现，会预先分配slice的大小。stack底层也是slice，默认大小是0，插入worker时，底层自动扩容。除了不同的管理队列实现外。也定义worker和queue需要的一些能力。如果可以的话，我们也可以实现一个queue用来管理worker。\n下面我们介绍下初始化pool时的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 //https://github.com/panjf2000/ants/blob/dev/pool.go func NewPool(size int, options ...Option) (*Pool, error) { //协程池没有限制大小 if size \u0026lt;= 0 { size = -1 } opts := loadOptions(options...) //配置是否需要清理worker的功能 if !opts.DisablePurge { if expiry := opts.ExpiryDuration; expiry \u0026lt; 0 { return nil, ErrInvalidPoolExpiry } else if expiry == 0 { opts.ExpiryDuration = DefaultCleanIntervalTime } } //用户可以自定义logger if opts.Logger == nil { opts.Logger = defaultLogger } //初始化一个pool结构p，然后创建worker的时候会传入这个p，所以所有的worker都是公用一个p，一个协程池的。 //具体看下workerCache.New方法就可以明白。 //task chan是有缓冲还是无缓冲取决于GOMAXPROCS，具体下面会解释 p := \u0026amp;Pool{ capacity: int32(size), lock: syncx.NewSpinLock(), options: opts, } p.workerCache.New = func() interface{} { return \u0026amp;goWorker{ pool: p, task: make(chan func(), workerChanCap), } } //这里是决定管理worker的队列使用ring queue还是stack。取决是否需要预分配 if p.options.PreAlloc { if size == -1 { return nil, ErrInvalidPreAllocSize } p.workers = newWorkerQueue(queueTypeLoopQueue, size) } else { p.workers = newWorkerQueue(queueTypeStack, 0) } //自己实现了个自旋锁，将这个锁作为cond条件变量的关联到这个锁 p.cond = sync.NewCond(p.lock) //清理程序和更新used time的定时任务 p.goPurge() p.goTicktock() return p, nil } //workerChanCap的容量取决于核的数量，单核时，我们设置chan为无缓冲，这样的话协程就会立刻从sender转化到receiver，提高程序性能。 //多核时，设置有缓冲的chan，避免接收方接受的速率影响到发送方 workerChanCap = func() int { // Use blocking channel if GOMAXPROCS=1. // This switches context from sender to receiver immediately, // which results in higher performance (under go1.5 at least). if runtime.GOMAXPROCS(0) == 1 { return 0 } // Use non-blocking workerChan if GOMAXPROCS\u0026gt;1, // since otherwise the sender might be dragged down if the receiver is CPU-bound. return 1 }() 初始化pool的函数中，我们可以配置是否使用purge功能清理woker，是否要自定义logger，是否要预分配workqueue等。比较重要的点就是使用对象池加速worker的分配，共用一个协程池对象p。根据是否预先分配，实现两种work queue，这些都是提升性能的关键点。还有就是自己实现了个自旋锁syncx.NewSpinLock()，加锁时会自旋尝试多次获取。\n扩展阅读 ","permalink":"https://www.youngergo.cn/en/posts/tech/go-ants/","summary":"简介 众所周知，golang的goroutine是go并发编程的基础知识，我们只需要使用简单go func()就可以开启一个go协程，然后调度器","title":"Go Ants源码解析"},{"content":"源码解析 源码版本：1.18\natomic包主要支持一些原子操作，首先我们来看看源码doc文件(atomic的源码路径：/src/runtime/internal/atomic)对atomic的介绍：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /* Package atomic provides atomic operations, independent of sync/atomic, to the runtime. On most platforms, the compiler is aware of the functions defined in this package, and they\u0026#39;re replaced with platform-specific intrinsics. On other platforms, generic implementations are made available. Unless otherwise noted, operations defined in this package are sequentially consistent across threads with respect to the Vals they manipulate. More specifically, operations that happen in a specific order on one thread, will always be observed to happen in exactly that order by another thread. */ /* 包atomic提供原子操作，独立于sync/atomic。向运行时提供原子操作。 在大多数平台上，编译器都知道这个包中定义的函数。 在这个包中定义的函数，它们被替换成特定平台的本征。 在其他平台上，通用的实现是可用的。 除非另有说明，本包中定义的操作在顺序上是 在它们所操作的值方面，在不同的线程中是一致的。更具体地说 具体来说，在一个线程上以特定顺序发生的操作。 将总是被另一个线程观察到以该顺序发生。 */ package atomic atomic 提供的是原子操作，atomic 包中支持六种类型\nint32 uint32 int64 uint64 uintptr unsafe.Pointer 每一个类型都支持以下操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //以Int32类型举例 // SwapInt32 atomically stores new into *addr and returns the previous *addr Val. func SwapInt32(addr *int32, new int32) (old int32); // SwapInt32 atomically stores new into *addr and returns the previous *addr Val. // 原子性的比较*addr和old，如果相同则将new赋值给*addr并返回真。 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool); // AddInt32 atomically adds delta to *addr and returns the new Val. func AddInt32(addr *int32, delta int32) (new int32); // LoadInt32 atomically loads *addr. func LoadInt32(addr *int32) (val int32); // StoreInt32 atomically stores val into *addr. func StoreInt32(addr *int32, val int32); 其中大部分的函数都是由汇编代码实现。源码包中根据系统平台的不同实现了不同的.s汇编代码。具体可查看/src/runtime/internal/atomic下的代码，我们举例 说明下amd64指令架构下的汇编实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // bool Cas(int32 *val, int32 old, int32 new) // Atomically: //\tif(*val == old){ //\t*val = new; //\treturn 1; //\t} else //\treturn 0; TEXT ·Cas(SB),NOSPLIT,$0-17 MOVQ\tptr+0(FP), BX MOVL\told+8(FP), AX MOVL\tnew+12(FP), CX LOCK CMPXCHGL\tCX, 0(BX) SETEQ\tret+16(FP) RET Cas函数主要对比val地址中的值是否和old一样，如果一样val赋值new，返回true，否则完成false。对应的汇编代码解释如下： 这段代码使用了汇编语言来进行内存操作，下面是对每一行代码的解释：\nMOVQ ptr+0(FP), BX：将变量 ptr 在帧指针 FP 上的偏移量为 0 的位置的值，即 ptr 所指向的内存地址的值，传送到寄存器 BX 中。\nMOVL old+8(FP), AX：将变量 old 在帧指针 FP 上的偏移量为 8 的位置的值，即 old 所指向的内存地址的值，传送到寄存器 AX 中。\nMOVL new+12(FP), CX：将变量 new 在帧指针 FP 上的偏移量为 12 的位置的值，即 new 所指向的内存地址的值，传送到寄存器 CX 中。\nLOCK：该指令是一个前缀指令，作用是在多核心处理器中保证对内存的原子性操作，即在对内存进行操作时，不会被其他处理器的操作中断。\nCMPXCHGL CX, 0(BX)：该指令是一个比较并交换指令，作用是将内存地址 BX 上的值与 AX 进行比较，如果相等，则将 CX 替换为内存地址 BX 上的值， 并返回成功的标记；如果不相等，则什么都不做，返回失败的标记。(关于CMPXCHGL指令的操作数，它有一个默认的操作者，即eax寄存器。也就是说，当CMPXCHGL指令执行时， 它会默认使用eax寄存器作为操作数，同时，它也需要至少一个内存地址作为操作数之一)\nSETEQ ret+16(FP)：如果比较并交换成功，则将变量 ret 在帧指针 FP 上的偏移量为 16 的位置的值设置为 1（标记成功； 如果比较并交换失败，则该指令什么也不做，即变量 ret 的值仍旧为 0。\nRET：返回函数并结束执行\n已上就是cas函数的实现过程。有几个点我们需要注意下。\nLOCK：是一个指令前缀，其后必须跟一条“读-改-写”性质的指令，它们可以是ADD, ADC, AND, BTC, BTR, BTS, CMPXCHG, CMPXCH8B, CMPXCHG16B, DEC, INC, NEG, NOT, OR, SBB, SUB, XOR, XADD, XCHG。该指令是一种锁定协议，用于封锁总线，禁止其他CPU对内存的操作来保证原子性。\nCMPXCHGL指令并不是个原子指令，在多核cpu下，还是需要lock这个指令来锁定的。lock之后的指令，会阻塞其他cpu的指令执行。具体实现是物理级别的总线锁， cpu在硬件层面支持原子操作。但是这个锁粒度非常大，很影响执行的效率。所以MESI协议诞生，它主要是解决多核cpu时对共享数据的访问控制。 尽管有LOCK前缀， 但如果对应数据已经在cache line里，也就不用锁定总线，仅锁住缓存行即可。(这里还涉及cpu缓存的知识可以学习耗子哥的这篇文章与程序员相关的CPU缓存知识)\natomic.Val 下面主要介绍下atomic.Val这个可以存储任何数据的函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // A Val provides an atomic load and store of a consistently typed Val. // The zero Val for a Val returns nil from Load. // Once Store has been called, a Val must not be copied. // // A Val must not be copied after first use. type Val struct { v any } // ifaceWords is interface{} internal representation. type ifaceWords struct { typ unsafe.Pointer data unsafe.Pointer } Val结构就是个interface类型，实际的数据存储是ifaceWords类型。typ指针保存存储对象的类型。data指针就是实际的数据。下面我们看下Load和Store方法的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 func (v *Val) Store(x interface{}) { // x为nil，直接panic if x == nil { panic(\u0026#34;sync/atomic: store of nil Val into Val\u0026#34;) } // 将现有的值和要写入的值转换为ifaceWords类型，这样下一步就能获取到它们的原始类型和真正的值 vp := (*ifaceWords)(unsafe.Pointer(v)) xp := (*ifaceWords)(unsafe.Pointer(\u0026amp;x)) for { // 获取现有的值的type typ := LoadPointer(\u0026amp;vp.typ) // 如果typ为nil说明这是第一次调用Store if typ == nil { // 如果是第一次调用，就占住当前的processor，不允许其他goroutine再抢， // runtime_procPin方法会先获取当前goroutine runtime_procPin() // 使用CAS操作，先尝试将typ设置为^uintptr(0)这个中间状态 // 如果失败，则证明已经有别的goroutine抢先完成了赋值操作 // 那它就解除抢占锁，继续循环等待 if !CompareAndSwapPointer(\u0026amp;vp.typ, nil, unsafe.Pointer(^uintptr(0))) { runtime_procUnpin() continue } // 如果设置成功，就原子性的更新对应的指针，最后解除抢占锁 StorePointer(\u0026amp;vp.data, xp.data) StorePointer(\u0026amp;vp.typ, xp.typ) runtime_procUnpin() return } // 如果typ为^uintptr(0)说明第一次写入还没有完成，继续循环等待 if uintptr(typ) == ^uintptr(0) { continue } // 如果要写入的类型和现有的类型不一致，则panic if typ != xp.typ { panic(\u0026#34;sync/atomic: store of inconsistently typed Val into Val\u0026#34;) } // 更新data，跳出循环 StorePointer(\u0026amp;vp.data, xp.data) return } } 上面代码讲解的非常清晰，简单来说，所谓的atomic.Val就像是interface一样，存储对象的类型和数据。然后通过指针，赋值操作管理对象。上面代码中有几个点需要注意：\nruntime_procPin、runtime_procUnpin函数 在 Golang 的运行时中，每个 goroutine 都是从可用的逻辑处理器中获取执行资源，它能够随时在这些逻辑处理器之间切换。但是在某些情况下， 我们需要将一个特定的 goroutine 固定在指定的当前线程上(M)运行，而 runtime_procPin 函数就是为了实现这一功能而设计的。具体实现时， runtime_procPin 函数会将当前 goroutine 绑定到指定的线程上，并将该线程置于固定调度模式下。这样可以确保指定的线程一直被用于执行该goroutine， 而不会被其他 goroutine 抢占。在运行完成后，使用 runtime_procUnpin 函数解除绑定。\nunsafe.Pointer(^uintptr(0))只是个中间状态，并发store时，用来判断初始化写入是否完成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (v *Val) Load() (x interface{}) { // 将*Val指针类型转换为*ifaceWords指针类型 vp := (*ifaceWords)(unsafe.Pointer(v)) // 原子性的获取到v的类型typ的指针 typ := LoadPointer(\u0026amp;vp.typ) // 如果没有写入或者正在写入，先返回，^uintptr(0)代表过渡状态，这和Store是对应的 if typ == nil || uintptr(typ) == ^uintptr(0) { return nil } // 原子性的获取到v的真正的值data的指针，然后返回 data := LoadPointer(\u0026amp;vp.data) xp := (*ifaceWords)(unsafe.Pointer(\u0026amp;x)) xp.typ = typ xp.data = data return } Load代码就比较简单了，对比Store相信你肯定能完全理解。\n参考 https://learnku.com/docs/learngo/1.0/atomic/14038 https://zhuanlan.zhihu.com/p/343563035 https://www.cyub.vip/2021/04/05/Golang%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%E4%B9%8Batomic%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/ https://blog.csdn.net/lotluck/article/details/78793468 https://coolshell.cn/articles/20793.html ","permalink":"https://www.youngergo.cn/en/posts/tech/go-atomic/","summary":"源码解析 源码版本：1.18 atomic包主要支持一些原子操作，首先我们来看看源码doc文件(atomic的源码路径：/src/runtime","title":"Go Atomic"},{"content":"Do not communicate by sharing memory;instead, share memory by communicate.不要通过共享内存来通信，相反，应该通过通信来共享内存。这是Go语言并发的哲学座右铭。每个go开发在学习channel之前，应该先理解这个原则。\n单纯地将函数并发执行是没有意义的。函数与函数间需要交换数据才能体现并发执行函数的意义。\n虽然可以使用共享内存进行数据交换，但是共享内存在不同的goroutine中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。\nGo语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\n如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。\nGo 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。(以上内容引自书籍并发编程对channel的解释)\n数据结构 发送数据的过程 接收数据的过程 如何安全的关闭chan Notification 1.mallocgc和malloc调用底层函数brk，bbrk做了什么\n2.源码中的quick path如何理解，可以快速排除异常情况\n3.sudoG是什么，双层缓存sudog是为了做什么\n4.gopark。goready大概做了什么\n5.如何安全的关闭chan\n如何安全的关闭chan 无缓冲的channel，同一个协程内读写，会导致all goroutine are asleep.dead lock 无缓冲的channel，通道的同步写早于读channel 从一个没有数据的channel里拿数据引起的死锁 循环等待引起的死锁，两个G互相持有对方拥有的资源，无法读写 有缓冲区，收发在同一个G，但是缓冲区已满，写阻塞 有缓冲区，读空的channel，读阻塞，可以加select控制 读写channel哪个先关。 关闭channel的原则，不要让receiver来关闭chan。也不要在多个sender的时候由sender关闭chan。会导致panic的情况有两个。一个是给已经关闭的chan写数据。另一个是重复close chan会导致panic。一些常见的方式，可以使用sync.Once和sync.mutex来关闭chan。还可以直接用panic和recovery来处理panic。\n一读一写(只要一个写都可以写端关闭)。写端关闭。不写数据的时候，关闭chan，通知读端。 一个读多个写。读端通知写端关闭。可以新加个close chan，通知写端不要输入了。 多读多写。增加toStop chan去通知关闭close chan。读写端会有select检查close chan的状态。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 func main() { rand.Seed(time.Now().UnixNano()) log.SetFlags(0) // ... const MaxRandomNumber = 100000 const NumReceivers = 10 const NumSenders = 1000 wgReceivers := sync.WaitGroup{} wgReceivers.Add(NumReceivers) // ... dataCh := make(chan int, 100) stopCh := make(chan struct{}) // stopCh is an additional signal channel. // Its sender is the moderator goroutine shown below. // Its reveivers are all senders and receivers of dataCh. toStop := make(chan string, 1) // the channel toStop is used to notify the moderator // to close the additional signal channel (stopCh). // Its senders are any senders and receivers of dataCh. // Its reveiver is the moderator goroutine shown below. var stoppedBy string // moderator go func() { stoppedBy = \u0026lt;-toStop // part of the trick used to notify the moderator // to close the additional signal channel. close(stopCh) }() // senders for i := 0; i \u0026lt; NumSenders; i++ { go func(id string) { for { Val := rand.Intn(MaxRandomNumber) if Val == 0 { // here, a trick is used to notify the moderator // to close the additional signal channel. select { case toStop \u0026lt;- \u0026#34;sender#\u0026#34; + id: default: } return } // the first select here is to try to exit the // goroutine as early as possible. select { case \u0026lt;-stopCh: return default: } select { case \u0026lt;-stopCh: return case dataCh \u0026lt;- Val: } } }(strconv.Itoa(i)) } // receivers for i := 0; i \u0026lt; NumReceivers; i++ { go func(id string) { defer wgReceivers.Done() for { // same as senders, the first select here is to // try to exit the goroutine as early as possible. select { case \u0026lt;-stopCh: return default: } select { case \u0026lt;-stopCh: return case Val := \u0026lt;-dataCh: if Val == MaxRandomNumber-1 { // the same trick is used to notify the moderator // to close the additional signal channel. select { case toStop \u0026lt;- \u0026#34;receiver#\u0026#34; + id: default: } return } log.Println(Val) } } }(strconv.Itoa(i)) } // ... wgReceivers.Wait() log.Println(\u0026#34;stopped by\u0026#34;, stoppedBy) } ","permalink":"https://www.youngergo.cn/en/posts/tech/go-chan/","summary":"Do not communicate by sharing memory;instead, share memory by communicate.不要通过共享内存来通信，相反，应该通过通信来共享内存。这是Go语言并发的哲学座右铭。每个go开发在","title":"Go Chan"},{"content":" 开端 今天学习下go里面的sync.mutex的实现以及相关扩展知识。\n锁的介绍 首先，计算机中的锁是为了控制并发情况下，对同一资源的并发访问。锁呢，有利有弊。好的点在于，我们可以控制并发访问的顺序逻辑。避免程序因为资源竞争，而出现一些预期外的情况。 不好的点在于，加锁意味着并发度的下降，效率的下降。所以我们在使用锁来完成业务需求的时候，也要考虑锁竞争对业务带来的影响。根据业务情况确定是否使用及使用的方式。\n数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // A Mutex is a mutual exclusion lock. // The zero Val for a Mutex is an unlocked mutex. // // A Mutex must not be copied after first use. type Mutex struct { state int32 sema uint32 } // A Locker represents an object that can be locked and unlocked. type Locker interface { Lock() Unlock() } const ( mutexLocked = 1 \u0026lt;\u0026lt; iota // mutex is locked mutexWoken mutexStarving mutexWaiterShift = iota // Mutex fairness. // // Mutex can be in 2 modes of operations: normal and starvation. // In normal mode waiters are queued in FIFO order, but a woken up waiter // does not own the mutex and competes with new arriving goroutines over // the ownership. New arriving goroutines have an advantage -- they are // already running on CPU and there can be lots of them, so a woken up // waiter has good chances of losing. In such case it is queued at front // of the wait queue. If a waiter fails to acquire the mutex for more than 1ms, // it switches mutex to the starvation mode. // // In starvation mode ownership of the mutex is directly handed off from // the unlocking goroutine to the waiter at the front of the queue. // New arriving goroutines don\u0026#39;t try to acquire the mutex even if it appears // to be unlocked, and don\u0026#39;t try to spin. Instead they queue themselves at // the tail of the wait queue. // // If a waiter receives ownership of the mutex and sees that either // (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms, // it switches mutex back to normal operation mode. // // Normal mode has considerably better performance as a goroutine can acquire // a mutex several times in a row even if there are blocked waiters. // Starvation mode is important to prevent pathological cases of tail latency. starvationThresholdNs = 1e6 ) 上面代码就是go1.18中sync.mutex的定义。可以看到Mutex结构体中有state和sema两个字段，\nstate int32类型，代表的是锁的状态. sema uint32类型，代表信号量。他主要用于唤醒阻塞在互斥锁上的其他协程。 锁的状态 图片来自Draveness大神，state有以下几种状态：\nmutexLocked。锁状态，占1bit，0-可以获取锁，1-锁定状态，阻塞等待 mutexWoken。唤醒状态。占1bit，代表一个过程阶段，0-没有协程唤醒 1-有协程被唤醒，申请锁定过程中。 mutexStarving。饥饿状态。占1bit，当协程超过1ms还没有获取锁时，锁就会处于饥饿状态。 mutexWaiterShift/waitersCount。等待信号量状态。占29bit，当有协程释放锁时，需要根据此状态，决定是否释放信号量。用于通知其他协程获取此锁。 实现原理 sync.Mutex的实现代码只有200多行，但是里面锁的切换控制还是比较复杂，下面我们逐步来分析\n加锁过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Lock locks m. // If the lock is already in use, the calling goroutine // blocks until the mutex is available. func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026amp;m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } Lock函数使用CompareAndSwapInt32判断m.state是不是0，如果是的话state设为lock状态，成功获取到锁。 失败则调用lockSlow()函数，这个函数是实现状态控制的主要逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 func (m *Mutex) lockSlow() { var waitStartTime int64 starving := false awoke := false iter := 0 old := m.state for { // Don\u0026#39;t spin in starvation mode, ownership is handed off to waiters // so we won\u0026#39;t be able to acquire the mutex anyway. if old\u0026amp;(mutexLocked|mutexStarving) == mutexLocked \u0026amp;\u0026amp; runtime_canSpin(iter) { // Active spinning makes sense. // Try to set mutexWoken flag to inform Unlock // to not wake other blocked goroutines. if !awoke \u0026amp;\u0026amp; old\u0026amp;mutexWoken == 0 \u0026amp;\u0026amp; old\u0026gt;\u0026gt;mutexWaiterShift != 0 \u0026amp;\u0026amp; atomic.CompareAndSwapInt32(\u0026amp;m.state, old, old|mutexWoken) { awoke = true } runtime_doSpin() iter++ old = m.state continue } new := old // Don\u0026#39;t try to acquire starving mutex, new arriving goroutines must queue. if old\u0026amp;mutexStarving == 0 { new |= mutexLocked } if old\u0026amp;(mutexLocked|mutexStarving) != 0 { new += 1 \u0026lt;\u0026lt; mutexWaiterShift } // The current goroutine switches mutex to starvation mode. // But if the mutex is currently unlocked, don\u0026#39;t do the switch. // Unlock expects that starving mutex has waiters, which will not // be true in this case. if starving \u0026amp;\u0026amp; old\u0026amp;mutexLocked != 0 { new |= mutexStarving } if awoke { // The goroutine has been woken from sleep, // so we need to reset the flag in either case. if new\u0026amp;mutexWoken == 0 { throw(\u0026#34;sync: inconsistent mutex state\u0026#34;) } new \u0026amp;^= mutexWoken } if atomic.CompareAndSwapInt32(\u0026amp;m.state, old, new) { if old\u0026amp;(mutexLocked|mutexStarving) == 0 { break // locked the mutex with CAS } // If we were already waiting before, queue at the front of the queue. queueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } runtime_SemacquireMutex(\u0026amp;m.sema, queueLifo, 1) starving = starving || runtime_nanotime()-waitStartTime \u0026gt; starvationThresholdNs old = m.state if old\u0026amp;mutexStarving != 0 { // If this goroutine was woken and mutex is in starvation mode, // ownership was handed off to us but mutex is in somewhat // inconsistent state: mutexLocked is not set and we are still // accounted as waiter. Fix that. if old\u0026amp;(mutexLocked|mutexWoken) != 0 || old\u0026gt;\u0026gt;mutexWaiterShift == 0 { throw(\u0026#34;sync: inconsistent mutex state\u0026#34;) } delta := int32(mutexLocked - 1\u0026lt;\u0026lt;mutexWaiterShift) if !starving || old\u0026gt;\u0026gt;mutexWaiterShift == 1 { // Exit starvation mode. // Critical to do it here and consider wait time. // Starvation mode is so inefficient, that two goroutines // can go lock-step infinitely once they switch mutex // to starvation mode. delta -= mutexStarving } atomic.AddInt32(\u0026amp;m.state, delta) break } awoke = true iter = 0 } else { old = m.state } } if race.Enabled { race.Acquire(unsafe.Pointer(m)) } } slowLock函数依靠CAS+信号量+自旋来实现。下面我们对照代码，逐行分析下逻辑：\n1.在调用slowLock之前，已经判断过state是否可以获取锁。进入slowLock后会阻塞当前G，尝试获取锁。 首先判断是否满足条件： 【锁处于非饥饿状态, locked状态，并且可以自旋】，满足即开始自旋，在自旋的过程中尝试将锁的状态设置为唤醒，尽量让当前G获取的锁。\n2.当在自旋的过程中发现可以获取锁时，进入下面逻辑。用old初始化一个临时的new state。判断锁如果不处于饥饿状态，new state加上locked状态。 (饥饿状态下的锁，G是需要排队才可以获取的，源码注释也有)。接着，判断old状态是locked或者饥饿时，将waiterShift加8，代表等待的G加1。 接着判断如果此G已经处于饥饿状态，并且old已经处于locked状态。new state就增加饥饿状态。(文中的翻译说明了原因，当前的goroutine将mutex设为饥饿状态， 但是如果mutex已经解锁的话，就不要进行设定了。因为Unlock需要处于饥饿状态的mutex有等待者)。 判断当前G是否是被唤醒的，是的话将new state设置成非唤醒。\n3.接下来要注意，这里再次将m.state和old进行了比较。主要是判断有其他的goroutine改变mutex的状态。如果有的话new state就作废，完成本次逻辑，继续循环自旋尝试获取锁。 如果没有的话，当前无锁+不饥饿就可以获取锁成功break。否则加锁失败，G需要进入队列，如果G第一次等待就放在队尾，否则就是被唤醒再次获取锁失败。就会被放在队首。 判断当前G是否该进入饥饿状态的标准是G等待mutex的时间是否大于1ms。重新获取mutex的状态，如果处于饥饿状态获取锁成功。此时需要考虑解除锁的饥饿状态。 满足1.当前G等待时间小于1ms 2.等待队列为空 两个条件其一即可退出饥饿模式。\n上面是对照代码的理解过程，下面我们总结下： 正常情况下, 当一个Goroutine获取到锁后, 其他的G获取锁时会自旋或者进入睡眠状态，等待信号量唤醒，自旋是希望当前的G能获取到锁，因为它持有CPU， 可以避免资源切换。但是又不能一直自旋， 所以mutex就会有饥饿模式，如果一个G被唤醒过后, 仍然没有拿到锁, 那么该G会放在等待队列的最前面。 并且那些等待超过1ms的G还没有获取到锁，该G就会进入饥饿状态。该G是饥饿状态并且Mutex是Locked状态时，才有可能给Mutex设置成饥饿状态。\n获取到锁的G解锁时，将mutex的状态设为unlock，然后发出信号，等待的G开始抢夺锁的。但是如果mutex处于饥饿状态，就会将信号发给第一个G，唤醒它。这就是G排队。\n解锁过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // Unlock unlocks m. // It is a run-time error if m is not locked on entry to Unlock. // // A locked Mutex is not associated with a particular goroutine. // It is allowed for one goroutine to lock a Mutex and then // arrange for another goroutine to unlock it. func (m *Mutex) Unlock() { if race.Enabled { _ = m.state race.Release(unsafe.Pointer(m)) } // Fast path: drop lock bit. new := atomic.AddInt32(\u0026amp;m.state, -mutexLocked) if new != 0 { // Outlined slow path to allow inlining the fast path. // To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock. m.unlockSlow(new) } } func (m *Mutex) unlockSlow(new int32) { if (new+mutexLocked)\u0026amp;mutexLocked == 0 { throw(\u0026#34;sync: unlock of unlocked mutex\u0026#34;) } if new\u0026amp;mutexStarving == 0 { old := new for { // If there are no waiters or a goroutine has already // been woken or grabbed the lock, no need to wake anyone. // In starvation mode ownership is directly handed off from unlocking // goroutine to the next waiter. We are not part of this chain, // since we did not observe mutexStarving when we unlocked the mutex above. // So get off the way. if old\u0026gt;\u0026gt;mutexWaiterShift == 0 || old\u0026amp;(mutexLocked|mutexWoken|mutexStarving) != 0 { return } // Grab the right to wake someone. new = (old - 1\u0026lt;\u0026lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(\u0026amp;m.state, old, new) { runtime_Semrelease(\u0026amp;m.sema, false, 1) return } old = m.state } } else { // Starving mode: handoff mutex ownership to the next waiter, and yield // our time slice so that the next waiter can start to run immediately. // Note: mutexLocked is not set, the waiter will set it after wakeup. // But mutex is still considered locked if mutexStarving is set, // so new coming goroutines won\u0026#39;t acquire it. runtime_Semrelease(\u0026amp;m.sema, true, 1) } } 解除mutex的lock状态，获取到最新的状态new。如果new是饥饿状态，唤醒第一个G，获取锁。解锁完成。 如果不是饥饿状态。当前没有G等待，或者有G已经被唤醒去加锁了。就不需要做唤醒的动作。退出即可。 否则将等待的G-1，并一直尝试将G设置为唤醒状态，释放信号量，通知所有的G都可以去抢夺锁。设置成功解锁完成，否则继续执行。\n引申问题 源码阅读注意的点 在理解sync.mutex的时候，一定要注意的点是，m的state在代码执行过程中，很可能会有其他的G改变其状态。所以查看代码逻辑的时候， 一定要时刻记得这个点\nwoken状态的意义 Woken状态用于加锁和解锁过程的通信，举个例子，同一时刻，两个协程一个在加锁，一个在解锁，在加锁的协程可能在自旋过程中， 此时把Woken标记为1，用于通知解锁协程不必释放信号量了，好比在说：你只管解锁好了，不必释放信号量，我马上就拿到锁了。\n为什么重复解锁会panic Unlock的逻辑是，解除mutex的lock状态，然后检查是否有协程等待，有的话释放信号量，唤醒协程。如果多次unlock的话，就会 发送多个信号量，唤醒多个G去抢夺锁。会造成不必要的竞争，也会造成协程切换，浪费资源，实现复杂度也会增加。\nG的饥饿和mutex饥饿的关系 只有G处于饥饿状态的时候，才会将mutex设为饥饿状态。当mutex处于饥饿状态时，才可能会让饥饿的G获取到锁。需要注意的是，设mutex 为饥饿状态的G不一定会获取到锁，有可能会被其他G截胡。\nG可以成功获取锁的情况 第一次加锁的时候m.state=0，一定是可以获取锁。没有其他的G获取锁，没有改变其状态。 当前的mutex不是饥饿状态，也不是lock状态，尝试CAS加锁的时候，如果没有其他G改变m状态，可以成功。 某个G被唤醒后，重新获取mutex，此时mutex处于饥饿状态，没有其他的G来抢夺，因为这个时候只唤醒了饥饿的G，G也可以成功。 参考资料 https://blog.csdn.net/baolingye/article/details/111357407 https://blog.csdn.net/qq_37102984/article/details/115322706 https://mp.weixin.qq.com/s/BZvfNn_Vre7o2T8BZ4LLMw ","permalink":"https://www.youngergo.cn/en/posts/tech/go-sync.mutex/","summary":"开端 今天学习下go里面的sync.mutex的实现以及相关扩展知识。 锁的介绍 首先，计算机中的锁是为了控制并发情况下，对同一资源的并发访问。锁","title":"Go sync.mutex源码解析"},{"content":"hi,我是younger，go后端开发。\n为什么建立自己Blog 人这一生呢，生不带来，去不带走的。总得留点东西在这个世界上，证明你来过。\n","permalink":"https://www.youngergo.cn/en/posts/life/me/","summary":"hi,我是younger，go后端开发。 为什么建立自己Blog 人这一生呢，生不带来，去不带走的。总得留点东西在这个世界上，证明你来过。","title":"Me"},{"content":"1.go-channel阻塞的场景\n无缓冲的channel，同一个协程内读写，会导致all goroutine are asleep.dead lock 无缓冲的channel，通道的同步写早于读channel 从一个没有数据的channel里拿数据引起的死锁 循环等待引起的死锁，两个G互相持有对方拥有的资源，无法读写 有缓冲区，收发在同一个G，但是缓冲区已满，写阻塞 有缓冲区，读空的channel，读阻塞，可以加select控制 2.读写channel哪个先关。\n关闭channel的原则，不要让receiver来关闭chan。也不要在多个sender的时候由sender关闭chan。会导致panic的情况有两个。一个是给已经关闭的chan写数据。另一个是重复close chan会导致panic。一些常见的方式，可以使用sync.Once和sync.mutex来关闭chan。还可以直接用panic和recovery来处理panic。\n一读一写(只要一个写都可以写端关闭)。写端关闭。不写数据的时候，关闭chan，通知读端。\n一个读多个写。读端通知写端关闭。可以新加个close chan，通知写端不要输入了。\n多读多写。增加toStop chan去通知关闭close chan。读写端会有select检查close chan的状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 func main() { rand.Seed(time.Now().UnixNano()) log.SetFlags(0) // ... const MaxRandomNumber = 100000 const NumReceivers = 10 const NumSenders = 1000 wgReceivers := sync.WaitGroup{} wgReceivers.Add(NumReceivers) // ... dataCh := make(chan int, 100) stopCh := make(chan struct{}) // stopCh is an additional signal channel. // Its sender is the moderator goroutine shown below. // Its reveivers are all senders and receivers of dataCh. toStop := make(chan string, 1) // the channel toStop is used to notify the moderator // to close the additional signal channel (stopCh). // Its senders are any senders and receivers of dataCh. // Its reveiver is the moderator goroutine shown below. var stoppedBy string // moderator go func() { stoppedBy = \u0026lt;-toStop // part of the trick used to notify the moderator // to close the additional signal channel. close(stopCh) }() // senders for i := 0; i \u0026lt; NumSenders; i++ { go func(id string) { for { Val := rand.Intn(MaxRandomNumber) if Val == 0 { // here, a trick is used to notify the moderator // to close the additional signal channel. select { case toStop \u0026lt;- \u0026#34;sender#\u0026#34; + id: default: } return } // the first select here is to try to exit the // goroutine as early as possible. select { case \u0026lt;-stopCh: return default: } select { case \u0026lt;-stopCh: return case dataCh \u0026lt;- Val: } } }(strconv.Itoa(i)) } // receivers for i := 0; i \u0026lt; NumReceivers; i++ { go func(id string) { defer wgReceivers.Done() for { // same as senders, the first select here is to // try to exit the goroutine as early as possible. select { case \u0026lt;-stopCh: return default: } select { case \u0026lt;-stopCh: return case Val := \u0026lt;-dataCh: if Val == MaxRandomNumber-1 { // the same trick is used to notify the moderator // to close the additional signal channel. select { case toStop \u0026lt;- \u0026#34;receiver#\u0026#34; + id: default: } return } log.Println(Val) } } }(strconv.Itoa(i)) } // ... wgReceivers.Wait() log.Println(\u0026#34;stopped by\u0026#34;, stoppedBy) } ","permalink":"https://www.youngergo.cn/en/posts/tech/go-interview/","summary":"1.go-channel阻塞的场景 无缓冲的channel，同一个协程内读写，会导致all goroutine are asleep.dead lock 无缓冲的channel，通道的同步写早于读c","title":""}]