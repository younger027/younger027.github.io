[{"content":"简介 众所周知，golang的goroutine是go并发编程的基础知识，我们只需要使用简单go func()就可以开启一个go协程，然后调度器会管理go协程分配相应的P和M，执行相应的业务逻辑，而且开启一个goroutine代价很小，只会占用2k的栈空间。但是协程也不是随意想开多少就多少，过多的goroutine不仅会影响调度器的执行时间，也会造成内存的快速增长。\n今天给大家分享一个github上有1w star的协程池库ants。\u0026ldquo;ants是一个高性能的 goroutine 池，实现了对大规模 goroutine 的调度管理、goroutine 复用，允许使用者在开发并发程序的时候限制 goroutine 数量，复用资源，达到更高效执行任务的效果。从benchmark的结果来看，使用ants的吞吐性能相较于原生 goroutine 可以保持在 2-6 倍的性能压制，而内存消耗则可以达到 10-20 倍的节省优势\u0026rdquo;(来自仓库描述)，具体的细节可以看git仓库。\n设计理念 ants的设计理念是使用N个work goroutine 完成M个task的执行。控制goroutine的数量。就像仓库中的Activity Diagrams描述的那样。但是这样的设计也会有一些弊端，我们后面再聊。 目录结构 ants的代码并不多，除了test外总共也只有十多个文件，而且代码行数也不多，下面是比较重要的几个文件：\nants.go ants.go 封装了外部调用的能力，它会初始化一个无 goroutine 限制的defaultAntsPool对象，提供了任务提交，获取运行协程数量，获取容量等能力。 pool.go pool.go 实现协程池需要的能力。协程池的定义，任务提交，分配协程等逻辑实现，都在这文件中，ants中的defaultAntsPool调用这个文件中的实现。 pool_func.go pool_func.go 和pool.go做的事情差不多，区别在于pool.go可以执行不同的协程任务，pool_func.go 执行的是相同的任务，但是可以传不同的参数。使用场景不同。 work.go work.go 完成具体任务的执行goWork，实例化了work_queue.go中worker interface的实现。和pool.go关联。 work_func.go work_func.go 和work.go的做了同样的事情，它和pool_func.go关联。 worker_queue.go worker_queue.go worker_queue是管理worker执行的队列，定义了worker 和worker queue需要实现的能力。 worker_loop_queue.go worker_loop_queue.go 用队列的形式实现对woker队列的管理。 worker_stack.go worker_stack.go 以栈的形式实现worker队列管理 options.go options.go ants对象的一些配置特性 这些就是ants的主要文件，他们之间的关系很简单，workerQueue interface定义了worker队列的能力，有ring queue和stack两种实现。woker interface定义了work的能力，有work和work_func两种实现。协程池也就有对应pool和pool_func两种实现。\n源码解析 ants的代码并不复杂，但是有很多细节，它实现了两种能力的协程池，pool是可以执行不同任务的协程池，poolWithFunc只能执行相同的任务，但是参数可以不同。对应此两种能力，所以有了goWorker和goWokerWithFunc两种worker。我们先介绍pool相关源码(pool关联work.go中的goWorker)，大家可以对照阅读poolWithFunc的代码。首先我们先从example入手：(一般开源的代码都会有example文件，我们学习之前可以先从这些示例入手，然后针对感兴趣的模块逐层深入，这样效率会高点)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 //https://github.com/panjf2000/ants/blob/dev/examples/main.go package main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;sync/atomic\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/panjf2000/ants/v2\u0026#34; ) var sum int32 func myFunc(i interface{}) { n := i.(int32) atomic.AddInt32(\u0026amp;sum, n) fmt.Printf(\u0026#34;run with %d\\n\u0026#34;, n) } func demoFunc() { time.Sleep(10 * time.Millisecond) fmt.Println(\u0026#34;Hello World!\u0026#34;) } func main() { defer ants.Release() runTimes := 1000 // Use the common pool. var wg sync.WaitGroup syncCalculateSum := func() { demoFunc() wg.Done() } for i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = ants.Submit(syncCalculateSum) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, ants.Running()) fmt.Printf(\u0026#34;finish all tasks.\\n\u0026#34;) // Use the pool with a method, // set 10 to the capacity of goroutine pool and 1 second for expired duration. p, _ := ants.NewPoolWithFunc(10, func(i interface{}) { myFunc(i) wg.Done() }) defer p.Release() // Submit tasks one by one. for i := 0; i \u0026lt; runTimes; i++ { wg.Add(1) _ = p.Invoke(int32(i)) } wg.Wait() fmt.Printf(\u0026#34;running goroutines: %d\\n\u0026#34;, p.Running()) fmt.Printf(\u0026#34;finish all tasks, result is %d\\n\u0026#34;, sum) if sum != 499500 { panic(\u0026#34;the final result is wrong!!!\u0026#34;) } } 测试代码介绍了pool和poolWithFunc的使用。syncCalculateSum函数就是我们需要执行的任务，ants.Submit(syncCalculateSum)将任务提交到pool中，然后会分配给work去执行。那么我们去看看Submit具体实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 //首先先了解下goWorker的定义 //https://github.com/panjf2000/ants/blob/dev/work.go type goWorker struct { // 协程池对象，其实多个goWorker共享的是一个pool对象，在创建worker是传的同一个p，下面代码会讲。 pool *Pool // 接受任务的有缓冲的chan task chan func() // 上次使用时间，主要是清理worker时使用 lastUsed time.Time } //https://github.com/panjf2000/ants/blob/dev/pool.go type Pool struct { // 协程池的容量，负数代表无限制 capacity int32 //当前在运行的worker数量 running int32 //保护从队列中读写worker lock sync.Locker // workers队列，这个队列有stack和ring queue两种实现 workers workerQueue //协程池状态 state int32 //自旋锁，等待获取空闲的worker cond *sync.Cond // workerCache 使用对象池，加快了在 function:retrieveWorker 中获取可用 Worker 的速度。 workerCache sync.Pool // 阻塞在cond上的任务数量 waiting int32 //清理等待时间过长的worker purgeDone int32 stopPurge context.CancelFunc //更新worker的使用时间，也是用于清理 ticktockDone int32 stopTicktock context.CancelFunc //使用时间 now atomic.Value //配置信息 options *Options } //https://github.com/panjf2000/ants/blob/dev/pool.go func (p *Pool) Submit(task func()) error { //判断协程池是否关闭 if p.IsClosed() { return ErrPoolClosed } //检索出一个可用的work，然后将任务交给work去执行 if w := p.retrieveWorker(); w != nil { w.inputFunc(task) return nil } return ErrPoolOverload //https://github.com/panjf2000/ants/blob/dev/pool.go func (p *Pool) retrieveWorker() (w worker) { //从缓存的对象池中获取一个worker，启动worker接受任务，这里可能会new一个，也可能复用 spawnWorker := func() { w = p.workerCache.Get().(*goWorker) w.run() } //加锁保护worker的获取 p.lock.Lock() w = p.workers.detach() if w != nil { //如果成功获取一个worker，直接返回 p.lock.Unlock() } else if capacity := p.Cap(); capacity == -1 || capacity \u0026gt; p.Running() { //如果初始化是未限制worker的数量，或者当前在运行的worker数量还没到达限制。就调用spawnWorker获取一个 p.lock.Unlock() spawnWorker() } else { //否则证明worker已经使用完了。如果用户设置了不阻塞，就直接返回nil，返回错误给用户 if p.options.Nonblocking { p.lock.Unlock() return } retry: //等待空闲的worker释放，如果等待的任务超出限制，直接返回 if p.options.MaxBlockingTasks != 0 \u0026amp;\u0026amp; p.Waiting() \u0026gt;= p.options.MaxBlockingTasks { p.lock.Unlock() return } //cond是实现的指数退避的自旋锁。自旋等待worker释放 p.addWaiting(1) p.cond.Wait() p.addWaiting(-1) //有worker可用或者是pool关闭了，关闭直接返回 if p.IsClosed() { p.lock.Unlock() return } //work可能被其他等待的任务获取了，如果还未到达容量上限，就从对象池拿一个 //否则重试继续获取 if w = p.workers.detach(); w == nil { if p.Free() \u0026gt; 0 { p.lock.Unlock() spawnWorker() return } goto retry } p.lock.Unlock() } //成功获取，返回 return } //https://github.com/panjf2000/ants/blob/dev/worker.go func (w *goWorker) inputFunc(fn func()) { //将任务丢给chan，会有worker去执行 w.task \u0026lt;- fn } func (w *goWorker) run() { w.pool.addRunning(1) go func() { //PanicHandler可以设置回调函数，gowoker panic的时候可以做些措施 defer func() { w.pool.addRunning(-1) w.pool.workerCache.Put(w) if p := recover(); p != nil { if ph := w.pool.options.PanicHandler; ph != nil { ph(p) } else { w.pool.options.Logger.Printf(\u0026#34;worker exits from panic: %v\\n%s\\n\u0026#34;, p, debug.Stack()) } } // Call Signal() here in case there are goroutines waiting for available workers. w.pool.cond.Signal() }() //每一个goworker都会启动一个协程从chan中读取任务执行，执行完成后将尝试将worker放到pool的队列中。 //如果放入失败，证明协程池中可运行协程数已满 or 当前协程池已经关闭，退出当前goroutine，将goworker放到缓存中。 //如果成功的话，证明当前协程池没满也没关闭，那这个协程就会阻塞在task chan上读取任务，此时这个协程是没有退出的 //goworker对象进入worker队列，等待新任务调度，有新的任务来时，等调度到goworker，然后将任务写入task chan for f := range w.task { if f == nil { return } f() if ok := w.pool.revertWorker(w); !ok { return } } }() } //尝试将goworker放到woker队列中管理 func (p *Pool) revertWorker(worker *goWorker) bool { //如果池子中运行的worker已经大于最大容量，或者池子已经关闭。返回放入失败 if capacity := p.Cap(); (capacity \u0026gt; 0 \u0026amp;\u0026amp; p.Running() \u0026gt; capacity) || p.IsClosed() { //这里的broadcast我查了资料，没有找到具体的原因，为什么放入失败需要唤醒所有等待的协程。 //我个人的理解是，唤醒retrieveWorker中所有阻塞在p.cond.wait()的协程，函数中会在retry时重新执行 //等待执行的任务是否超过用户设置的MaxBlockingTasks的逻辑。 p.cond.Broadcast() return false } //更新此worker使用时间 worker.lastUsed = p.nowTime() p.lock.Lock() // 这里为了避免内存泄漏，二次检查池的状态。具体泄漏场景大家可以看看下面的issue // Issue: https://github.com/panjf2000/ants/issues/113 if p.IsClosed() { p.lock.Unlock() return false } //调用worker queue的方法，将worker放入队列管理 if err := p.workers.insert(worker); err != nil { p.lock.Unlock() return false } //通知所有阻塞在\u0026#39;retrieveWorker()\u0026#39; 方法的协程，有新的worker可用 p.cond.Signal() p.lock.Unlock() return true } 整个流程比较简单，调用submit函数提交自定义的任务给协程池，submit调用retrieveWorker获取一个可用的worker，然后在调用inputFunc函数将任务交给这个worker去执行。有几个点需要注意下，workerCache使用了对象池，缓存了goworker对象，从池中Get时，可能会new一个woker(程序刚启动时，还未到达cap限制)，也可能会读取使用过的gowoker。还有就是使用了自旋锁sync.Cond等待可用的worker。还有个比较重要的是goworker的run函数，每一个goworker都会启动一个协程从chan中读取任务来执行。每次执行一个任务，都会尝试将此worker重新放入pool的队列中，等待再次调度。如果放入失败，就会放到workerCache对象池，最大程度复用对象。\n上面就是整个worker调用和分配的的流程，整体逻辑不是很复杂，但是有很多细节需要注意。下面顺着上面的逻辑，我们介绍下worker_queue\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 //https://github.com/panjf2000/ants/blob/dev/worker_queue.go type worker interface { run() finish() lastUsedTime() time.Time inputFunc(func()) inputParam(interface{}) } type workerQueue interface { len() int isEmpty() bool insert(worker) error detach() worker refresh(duration time.Duration) []worker reset() } type queueType int const ( queueTypeStack queueType = 1 \u0026lt;\u0026lt; iota queueTypeLoopQueue ) func newWorkerQueue(qType queueType, size int) workerQueue { switch qType { case queueTypeStack: return newWorkerStack(size) case queueTypeLoopQueue: return newWorkerLoopQueue(size) default: return newWorkerStack(size) } Pool对象中有workers workerQueue对象，workerQueue管理所有的worker，根据newWorkerQueue方法，我们可以发现workerQueue有两种实现，loopQueue实现了ring queue，底层是slice数组加一些头尾标识来实现，会预先分配slice的大小。stack底层也是slice，默认大小是0，插入worker时，底层自动扩容。除了不同的管理队列实现外。也定义worker和queue需要的一些能力。如果可以的话，我们也可以实现一个queue用来管理worker。\n下面我们介绍下初始化pool时的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 //https://github.com/panjf2000/ants/blob/dev/pool.go func NewPool(size int, options ...Option) (*Pool, error) { //协程池没有限制大小 if size \u0026lt;= 0 { size = -1 } opts := loadOptions(options...) //配置是否需要清理worker的功能 if !opts.DisablePurge { if expiry := opts.ExpiryDuration; expiry \u0026lt; 0 { return nil, ErrInvalidPoolExpiry } else if expiry == 0 { opts.ExpiryDuration = DefaultCleanIntervalTime } } //用户可以自定义logger if opts.Logger == nil { opts.Logger = defaultLogger } //初始化一个pool结构p，然后创建worker的时候会传入这个p，所以所有的worker都是公用一个p，一个协程池的。 //具体看下workerCache.New方法就可以明白。 //task chan是有缓冲还是无缓冲取决于GOMAXPROCS，具体下面会解释 p := \u0026amp;Pool{ capacity: int32(size), lock: syncx.NewSpinLock(), options: opts, } p.workerCache.New = func() interface{} { return \u0026amp;goWorker{ pool: p, task: make(chan func(), workerChanCap), } } //这里是决定管理worker的队列使用ring queue还是stack。取决是否需要预分配 if p.options.PreAlloc { if size == -1 { return nil, ErrInvalidPreAllocSize } p.workers = newWorkerQueue(queueTypeLoopQueue, size) } else { p.workers = newWorkerQueue(queueTypeStack, 0) } //初始化自旋锁，清理程序和更新used time的定时任务 p.cond = sync.NewCond(p.lock) p.goPurge() p.goTicktock() return p, nil } //workerChanCap的容量取决于核的数量，单核时，我们设置chan为无缓冲，这样的话协程就会立刻从sender转化到receiver，提高程序性能。 //多核时，设置有缓冲的chan，避免接收方接受的速率影响到发送方 workerChanCap = func() int { // Use blocking channel if GOMAXPROCS=1. // This switches context from sender to receiver immediately, // which results in higher performance (under go1.5 at least). if runtime.GOMAXPROCS(0) == 1 { return 0 } // Use non-blocking workerChan if GOMAXPROCS\u0026gt;1, // since otherwise the sender might be dragged down if the receiver is CPU-bound. return 1 }() 扩展阅读 ","permalink":"https://younger027.github.io/en/posts/tech/go-ants/","summary":"简介 众所周知，golang的goroutine是go并发编程的基础知识，我们只需要使用简单go func()就可以开启一个go协程，然后调度器","title":"Go Ants"},{"content":"源码解析 源码版本：1.18\natomic包主要支持一些原子操作，首先我们来看看源码doc文件(atomic的源码路径：/src/runtime/internal/atomic)对atomic的介绍：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /* Package atomic provides atomic operations, independent of sync/atomic, to the runtime. On most platforms, the compiler is aware of the functions defined in this package, and they\u0026#39;re replaced with platform-specific intrinsics. On other platforms, generic implementations are made available. Unless otherwise noted, operations defined in this package are sequentially consistent across threads with respect to the Vals they manipulate. More specifically, operations that happen in a specific order on one thread, will always be observed to happen in exactly that order by another thread. */ /* 包atomic提供原子操作，独立于sync/atomic。向运行时提供原子操作。 在大多数平台上，编译器都知道这个包中定义的函数。 在这个包中定义的函数，它们被替换成特定平台的本征。 在其他平台上，通用的实现是可用的。 除非另有说明，本包中定义的操作在顺序上是 在它们所操作的值方面，在不同的线程中是一致的。更具体地说 具体来说，在一个线程上以特定顺序发生的操作。 将总是被另一个线程观察到以该顺序发生。 */ package atomic atomic 提供的是原子操作，atomic 包中支持六种类型\nint32 uint32 int64 uint64 uintptr unsafe.Pointer 每一个类型都支持以下操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 //以Int32类型举例 // SwapInt32 atomically stores new into *addr and returns the previous *addr Val. func SwapInt32(addr *int32, new int32) (old int32); // SwapInt32 atomically stores new into *addr and returns the previous *addr Val. // 原子性的比较*addr和old，如果相同则将new赋值给*addr并返回真。 func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool); // AddInt32 atomically adds delta to *addr and returns the new Val. func AddInt32(addr *int32, delta int32) (new int32); // LoadInt32 atomically loads *addr. func LoadInt32(addr *int32) (val int32); // StoreInt32 atomically stores val into *addr. func StoreInt32(addr *int32, val int32); 其中大部分的函数都是由汇编代码实现。源码包中根据系统平台的不同实现了不同的.s汇编代码。具体可查看/src/runtime/internal/atomic下的代码，我们举例 说明下amd64指令架构下的汇编实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // bool Cas(int32 *val, int32 old, int32 new) // Atomically: //\tif(*val == old){ //\t*val = new; //\treturn 1; //\t} else //\treturn 0; TEXT ·Cas(SB),NOSPLIT,$0-17 MOVQ\tptr+0(FP), BX MOVL\told+8(FP), AX MOVL\tnew+12(FP), CX LOCK CMPXCHGL\tCX, 0(BX) SETEQ\tret+16(FP) RET Cas函数主要对比val地址中的值是否和old一样，如果一样val赋值new，返回true，否则完成false。对应的汇编代码解释如下： 这段代码使用了汇编语言来进行内存操作，下面是对每一行代码的解释：\nMOVQ ptr+0(FP), BX：将变量 ptr 在帧指针 FP 上的偏移量为 0 的位置的值，即 ptr 所指向的内存地址的值，传送到寄存器 BX 中。\nMOVL old+8(FP), AX：将变量 old 在帧指针 FP 上的偏移量为 8 的位置的值，即 old 所指向的内存地址的值，传送到寄存器 AX 中。\nMOVL new+12(FP), CX：将变量 new 在帧指针 FP 上的偏移量为 12 的位置的值，即 new 所指向的内存地址的值，传送到寄存器 CX 中。\nLOCK：该指令是一个前缀指令，作用是在多核心处理器中保证对内存的原子性操作，即在对内存进行操作时，不会被其他处理器的操作中断。\nCMPXCHGL CX, 0(BX)：该指令是一个比较并交换指令，作用是将内存地址 BX 上的值与 AX 进行比较，如果相等，则将 CX 替换为内存地址 BX 上的值， 并返回成功的标记；如果不相等，则什么都不做，返回失败的标记。(关于CMPXCHGL指令的操作数，它有一个默认的操作者，即eax寄存器。也就是说，当CMPXCHGL指令执行时， 它会默认使用eax寄存器作为操作数，同时，它也需要至少一个内存地址作为操作数之一)\nSETEQ ret+16(FP)：如果比较并交换成功，则将变量 ret 在帧指针 FP 上的偏移量为 16 的位置的值设置为 1（标记成功； 如果比较并交换失败，则该指令什么也不做，即变量 ret 的值仍旧为 0。\nRET：返回函数并结束执行\n已上就是cas函数的实现过程。有几个点我们需要注意下。\nLOCK：是一个指令前缀，其后必须跟一条“读-改-写”性质的指令，它们可以是ADD, ADC, AND, BTC, BTR, BTS, CMPXCHG, CMPXCH8B, CMPXCHG16B, DEC, INC, NEG, NOT, OR, SBB, SUB, XOR, XADD, XCHG。该指令是一种锁定协议，用于封锁总线，禁止其他CPU对内存的操作来保证原子性。\nCMPXCHGL指令并不是个原子指令，在多核cpu下，还是需要lock这个指令来锁定的。lock之后的指令，会阻塞其他cpu的指令执行。具体实现是物理级别的总线锁， cpu在硬件层面支持原子操作。但是这个锁粒度非常大，很影响执行的效率。所以MESI协议诞生，它主要是解决多核cpu时对共享数据的访问控制。 尽管有LOCK前缀， 但如果对应数据已经在cache line里，也就不用锁定总线，仅锁住缓存行即可。(这里还涉及cpu缓存的知识可以学习耗子哥的这篇文章与程序员相关的CPU缓存知识)\natomic.Val 下面主要介绍下atomic.Val这个可以存储任何数据的函数。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // A Val provides an atomic load and store of a consistently typed Val. // The zero Val for a Val returns nil from Load. // Once Store has been called, a Val must not be copied. // // A Val must not be copied after first use. type Val struct { v any } // ifaceWords is interface{} internal representation. type ifaceWords struct { typ unsafe.Pointer data unsafe.Pointer } Val结构就是个interface类型，实际的数据存储是ifaceWords类型。typ指针保存存储对象的类型。data指针就是实际的数据。下面我们看下Load和Store方法的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 func (v *Val) Store(x interface{}) { // x为nil，直接panic if x == nil { panic(\u0026#34;sync/atomic: store of nil Val into Val\u0026#34;) } // 将现有的值和要写入的值转换为ifaceWords类型，这样下一步就能获取到它们的原始类型和真正的值 vp := (*ifaceWords)(unsafe.Pointer(v)) xp := (*ifaceWords)(unsafe.Pointer(\u0026amp;x)) for { // 获取现有的值的type typ := LoadPointer(\u0026amp;vp.typ) // 如果typ为nil说明这是第一次调用Store if typ == nil { // 如果是第一次调用，就占住当前的processor，不允许其他goroutine再抢， // runtime_procPin方法会先获取当前goroutine runtime_procPin() // 使用CAS操作，先尝试将typ设置为^uintptr(0)这个中间状态 // 如果失败，则证明已经有别的goroutine抢先完成了赋值操作 // 那它就解除抢占锁，继续循环等待 if !CompareAndSwapPointer(\u0026amp;vp.typ, nil, unsafe.Pointer(^uintptr(0))) { runtime_procUnpin() continue } // 如果设置成功，就原子性的更新对应的指针，最后解除抢占锁 StorePointer(\u0026amp;vp.data, xp.data) StorePointer(\u0026amp;vp.typ, xp.typ) runtime_procUnpin() return } // 如果typ为^uintptr(0)说明第一次写入还没有完成，继续循环等待 if uintptr(typ) == ^uintptr(0) { continue } // 如果要写入的类型和现有的类型不一致，则panic if typ != xp.typ { panic(\u0026#34;sync/atomic: store of inconsistently typed Val into Val\u0026#34;) } // 更新data，跳出循环 StorePointer(\u0026amp;vp.data, xp.data) return } } 上面代码讲解的非常清晰，简单来说，所谓的atomic.Val就像是interface一样，存储对象的类型和数据。然后通过指针，赋值操作管理对象。上面代码中有几个点需要注意：\nruntime_procPin、runtime_procUnpin函数 在 Golang 的运行时中，每个 goroutine 都是从可用的逻辑处理器中获取执行资源，它能够随时在这些逻辑处理器之间切换。但是在某些情况下， 我们需要将一个特定的 goroutine 固定在指定的当前线程上(M)运行，而 runtime_procPin 函数就是为了实现这一功能而设计的。具体实现时， runtime_procPin 函数会将当前 goroutine 绑定到指定的线程上，并将该线程置于固定调度模式下。这样可以确保指定的线程一直被用于执行该goroutine， 而不会被其他 goroutine 抢占。在运行完成后，使用 runtime_procUnpin 函数解除绑定。\nunsafe.Pointer(^uintptr(0))只是个中间状态，并发store时，用来判断初始化写入是否完成。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (v *Val) Load() (x interface{}) { // 将*Val指针类型转换为*ifaceWords指针类型 vp := (*ifaceWords)(unsafe.Pointer(v)) // 原子性的获取到v的类型typ的指针 typ := LoadPointer(\u0026amp;vp.typ) // 如果没有写入或者正在写入，先返回，^uintptr(0)代表过渡状态，这和Store是对应的 if typ == nil || uintptr(typ) == ^uintptr(0) { return nil } // 原子性的获取到v的真正的值data的指针，然后返回 data := LoadPointer(\u0026amp;vp.data) xp := (*ifaceWords)(unsafe.Pointer(\u0026amp;x)) xp.typ = typ xp.data = data return } Load代码就比较简单了，对比Store相信你肯定能完全理解。\n参考 https://learnku.com/docs/learngo/1.0/atomic/14038 https://zhuanlan.zhihu.com/p/343563035 https://www.cyub.vip/2021/04/05/Golang%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E7%B3%BB%E5%88%97%E4%B9%8Batomic%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0/ https://blog.csdn.net/lotluck/article/details/78793468 https://coolshell.cn/articles/20793.html ","permalink":"https://younger027.github.io/en/posts/tech/go-atomic/","summary":"源码解析 源码版本：1.18 atomic包主要支持一些原子操作，首先我们来看看源码doc文件(atomic的源码路径：/src/runtime","title":"Go Atomic"},{"content":"Do not communicate by sharing memory;instead, share memory by communicate.不要通过共享内存来通信，相反，应该通过通信来共享内存。这是Go语言并发的哲学座右铭。每个go开发在学习channel之前，应该先理解这个原则。\n单纯地将函数并发执行是没有意义的。函数与函数间需要交换数据才能体现并发执行函数的意义。\n虽然可以使用共享内存进行数据交换，但是共享内存在不同的goroutine中容易发生竞态问题。为了保证数据交换的正确性，必须使用互斥量对内存进行加锁，这种做法势必造成性能问题。\nGo语言的并发模型是CSP（Communicating Sequential Processes），提倡通过通信共享内存而不是通过共享内存而实现通信。\n如果说goroutine是Go程序并发的执行体，channel就是它们之间的连接。channel是可以让一个goroutine发送特定值到另一个goroutine的通信机制。\nGo 语言中的通道（channel）是一种特殊的类型。通道像一个传送带或者队列，总是遵循先入先出（First In First Out）的规则，保证收发数据的顺序。每一个通道都是一个具体类型的导管，也就是声明channel的时候需要为其指定元素类型。(以上内容引自书籍并发编程对channel的解释)\n数据结构 发送数据的过程 接收数据的过程 如何安全的关闭chan Notification 1.mallocgc和malloc调用底层函数brk，bbrk做了什么\n2.源码中的quick path如何理解，可以快速排除异常情况\n3.sudoG是什么，双层缓存sudog是为了做什么\n4.gopark。goready大概做了什么\n5.如何安全的关闭chan\n","permalink":"https://younger027.github.io/en/posts/tech/go-chan/","summary":"Do not communicate by sharing memory;instead, share memory by communicate.不要通过共享内存来通信，相反，应该通过通信来共享内存。这是Go语言并发的哲学座右铭。每个go开发在","title":"Go Chan"},{"content":" 开端 今天学习下go里面的sync.mutex的实现以及相关扩展知识。\n锁的介绍 首先，计算机中的锁是为了控制并发情况下，对同一资源的并发访问。锁呢，有利有弊。好的点在于，我们可以控制并发访问的顺序逻辑。避免程序因为资源竞争，而出现一些预期外的情况。 不好的点在于，加锁意味着并发度的下降，效率的下降。所以我们在使用锁来完成业务需求的时候，也要考虑锁竞争对业务带来的影响。根据业务情况确定是否使用及使用的方式。\n数据结构 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 // A Mutex is a mutual exclusion lock. // The zero Val for a Mutex is an unlocked mutex. // // A Mutex must not be copied after first use. type Mutex struct { state int32 sema uint32 } // A Locker represents an object that can be locked and unlocked. type Locker interface { Lock() Unlock() } const ( mutexLocked = 1 \u0026lt;\u0026lt; iota // mutex is locked mutexWoken mutexStarving mutexWaiterShift = iota // Mutex fairness. // // Mutex can be in 2 modes of operations: normal and starvation. // In normal mode waiters are queued in FIFO order, but a woken up waiter // does not own the mutex and competes with new arriving goroutines over // the ownership. New arriving goroutines have an advantage -- they are // already running on CPU and there can be lots of them, so a woken up // waiter has good chances of losing. In such case it is queued at front // of the wait queue. If a waiter fails to acquire the mutex for more than 1ms, // it switches mutex to the starvation mode. // // In starvation mode ownership of the mutex is directly handed off from // the unlocking goroutine to the waiter at the front of the queue. // New arriving goroutines don\u0026#39;t try to acquire the mutex even if it appears // to be unlocked, and don\u0026#39;t try to spin. Instead they queue themselves at // the tail of the wait queue. // // If a waiter receives ownership of the mutex and sees that either // (1) it is the last waiter in the queue, or (2) it waited for less than 1 ms, // it switches mutex back to normal operation mode. // // Normal mode has considerably better performance as a goroutine can acquire // a mutex several times in a row even if there are blocked waiters. // Starvation mode is important to prevent pathological cases of tail latency. starvationThresholdNs = 1e6 ) 上面代码就是go1.18中sync.mutex的定义。可以看到Mutex结构体中有state和sema两个字段，\nstate int32类型，代表的是锁的状态. sema uint32类型，代表信号量。他主要用于唤醒阻塞在互斥锁上的其他协程。 锁的状态 图片来自Draveness大神，state有以下几种状态：\nmutexLocked。锁状态，占1bit，0-可以获取锁，1-锁定状态，阻塞等待 mutexWoken。唤醒状态。占1bit，代表一个过程阶段，0-没有协程唤醒 1-有协程被唤醒，申请锁定过程中。 mutexStarving。饥饿状态。占1bit，当协程超过1ms还没有获取锁时，锁就会处于饥饿状态。 mutexWaiterShift/waitersCount。等待信号量状态。占29bit，当有协程释放锁时，需要根据此状态，决定是否释放信号量。用于通知其他协程获取此锁。 实现原理 sync.Mutex的实现代码只有200多行，但是里面锁的切换控制还是比较复杂，下面我们逐步来分析\n加锁过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // Lock locks m. // If the lock is already in use, the calling goroutine // blocks until the mutex is available. func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026amp;m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } Lock函数使用CompareAndSwapInt32判断m.state是不是0，如果是的话state设为lock状态，成功获取到锁。 失败则调用lockSlow()函数，这个函数是实现状态控制的主要逻辑。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 func (m *Mutex) lockSlow() { var waitStartTime int64 starving := false awoke := false iter := 0 old := m.state for { // Don\u0026#39;t spin in starvation mode, ownership is handed off to waiters // so we won\u0026#39;t be able to acquire the mutex anyway. if old\u0026amp;(mutexLocked|mutexStarving) == mutexLocked \u0026amp;\u0026amp; runtime_canSpin(iter) { // Active spinning makes sense. // Try to set mutexWoken flag to inform Unlock // to not wake other blocked goroutines. if !awoke \u0026amp;\u0026amp; old\u0026amp;mutexWoken == 0 \u0026amp;\u0026amp; old\u0026gt;\u0026gt;mutexWaiterShift != 0 \u0026amp;\u0026amp; atomic.CompareAndSwapInt32(\u0026amp;m.state, old, old|mutexWoken) { awoke = true } runtime_doSpin() iter++ old = m.state continue } new := old // Don\u0026#39;t try to acquire starving mutex, new arriving goroutines must queue. if old\u0026amp;mutexStarving == 0 { new |= mutexLocked } if old\u0026amp;(mutexLocked|mutexStarving) != 0 { new += 1 \u0026lt;\u0026lt; mutexWaiterShift } // The current goroutine switches mutex to starvation mode. // But if the mutex is currently unlocked, don\u0026#39;t do the switch. // Unlock expects that starving mutex has waiters, which will not // be true in this case. if starving \u0026amp;\u0026amp; old\u0026amp;mutexLocked != 0 { new |= mutexStarving } if awoke { // The goroutine has been woken from sleep, // so we need to reset the flag in either case. if new\u0026amp;mutexWoken == 0 { throw(\u0026#34;sync: inconsistent mutex state\u0026#34;) } new \u0026amp;^= mutexWoken } if atomic.CompareAndSwapInt32(\u0026amp;m.state, old, new) { if old\u0026amp;(mutexLocked|mutexStarving) == 0 { break // locked the mutex with CAS } // If we were already waiting before, queue at the front of the queue. queueLifo := waitStartTime != 0 if waitStartTime == 0 { waitStartTime = runtime_nanotime() } runtime_SemacquireMutex(\u0026amp;m.sema, queueLifo, 1) starving = starving || runtime_nanotime()-waitStartTime \u0026gt; starvationThresholdNs old = m.state if old\u0026amp;mutexStarving != 0 { // If this goroutine was woken and mutex is in starvation mode, // ownership was handed off to us but mutex is in somewhat // inconsistent state: mutexLocked is not set and we are still // accounted as waiter. Fix that. if old\u0026amp;(mutexLocked|mutexWoken) != 0 || old\u0026gt;\u0026gt;mutexWaiterShift == 0 { throw(\u0026#34;sync: inconsistent mutex state\u0026#34;) } delta := int32(mutexLocked - 1\u0026lt;\u0026lt;mutexWaiterShift) if !starving || old\u0026gt;\u0026gt;mutexWaiterShift == 1 { // Exit starvation mode. // Critical to do it here and consider wait time. // Starvation mode is so inefficient, that two goroutines // can go lock-step infinitely once they switch mutex // to starvation mode. delta -= mutexStarving } atomic.AddInt32(\u0026amp;m.state, delta) break } awoke = true iter = 0 } else { old = m.state } } if race.Enabled { race.Acquire(unsafe.Pointer(m)) } } slowLock函数依靠CAS+信号量+自旋来实现。下面我们对照代码，逐行分析下逻辑：\n1.在调用slowLock之前，已经判断过state是否可以获取锁。进入slowLock后会阻塞当前G，尝试获取锁。 首先判断是否满足条件： 【锁处于非饥饿状态, locked状态，并且可以自旋】，满足即开始自旋，在自旋的过程中尝试将锁的状态设置为唤醒，尽量让当前G获取的锁。\n2.当在自旋的过程中发现可以获取锁时，进入下面逻辑。用old初始化一个临时的new state。判断锁如果不处于饥饿状态，new state加上locked状态。 (饥饿状态下的锁，G是需要排队才可以获取的，源码注释也有)。接着，判断old状态是locked或者饥饿时，将waiterShift加8，代表等待的G加1。 接着判断如果此G已经处于饥饿状态，并且old已经处于locked状态。new state就增加饥饿状态。(文中的翻译说明了原因，当前的goroutine将mutex设为饥饿状态， 但是如果mutex已经解锁的话，就不要进行设定了。因为Unlock需要处于饥饿状态的mutex有等待者)。 判断当前G是否是被唤醒的，是的话将new state设置成非唤醒。\n3.接下来要注意，这里再次将m.state和old进行了比较。主要是判断有其他的goroutine改变mutex的状态。如果有的话new state就作废，完成本次逻辑，继续循环自旋尝试获取锁。 如果没有的话，当前无锁+不饥饿就可以获取锁成功break。否则加锁失败，G需要进入队列，如果G第一次等待就放在队尾，否则就是被唤醒再次获取锁失败。就会被放在队首。 判断当前G是否该进入饥饿状态的标准是G等待mutex的时间是否大于1ms。重新获取mutex的状态，如果处于饥饿状态获取锁成功。此时需要考虑解除锁的饥饿状态。 满足1.当前G等待时间小于1ms 2.等待队列为空 两个条件其一即可退出饥饿模式。\n上面是对照代码的理解过程，下面我们总结下： 正常情况下, 当一个Goroutine获取到锁后, 其他的G获取锁时会自旋或者进入睡眠状态，等待信号量唤醒，自旋是希望当前的G能获取到锁，因为它持有CPU， 可以避免资源切换。但是又不能一直自旋， 所以mutex就会有饥饿模式，如果一个G被唤醒过后, 仍然没有拿到锁, 那么该G会放在等待队列的最前面。 并且那些等待超过1ms的G还没有获取到锁，该G就会进入饥饿状态。该G是饥饿状态并且Mutex是Locked状态时，才有可能给Mutex设置成饥饿状态。\n获取到锁的G解锁时，将mutex的状态设为unlock，然后发出信号，等待的G开始抢夺锁的。但是如果mutex处于饥饿状态，就会将信号发给第一个G，唤醒它。这就是G排队。\n解锁过程 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 // Unlock unlocks m. // It is a run-time error if m is not locked on entry to Unlock. // // A locked Mutex is not associated with a particular goroutine. // It is allowed for one goroutine to lock a Mutex and then // arrange for another goroutine to unlock it. func (m *Mutex) Unlock() { if race.Enabled { _ = m.state race.Release(unsafe.Pointer(m)) } // Fast path: drop lock bit. new := atomic.AddInt32(\u0026amp;m.state, -mutexLocked) if new != 0 { // Outlined slow path to allow inlining the fast path. // To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock. m.unlockSlow(new) } } func (m *Mutex) unlockSlow(new int32) { if (new+mutexLocked)\u0026amp;mutexLocked == 0 { throw(\u0026#34;sync: unlock of unlocked mutex\u0026#34;) } if new\u0026amp;mutexStarving == 0 { old := new for { // If there are no waiters or a goroutine has already // been woken or grabbed the lock, no need to wake anyone. // In starvation mode ownership is directly handed off from unlocking // goroutine to the next waiter. We are not part of this chain, // since we did not observe mutexStarving when we unlocked the mutex above. // So get off the way. if old\u0026gt;\u0026gt;mutexWaiterShift == 0 || old\u0026amp;(mutexLocked|mutexWoken|mutexStarving) != 0 { return } // Grab the right to wake someone. new = (old - 1\u0026lt;\u0026lt;mutexWaiterShift) | mutexWoken if atomic.CompareAndSwapInt32(\u0026amp;m.state, old, new) { runtime_Semrelease(\u0026amp;m.sema, false, 1) return } old = m.state } } else { // Starving mode: handoff mutex ownership to the next waiter, and yield // our time slice so that the next waiter can start to run immediately. // Note: mutexLocked is not set, the waiter will set it after wakeup. // But mutex is still considered locked if mutexStarving is set, // so new coming goroutines won\u0026#39;t acquire it. runtime_Semrelease(\u0026amp;m.sema, true, 1) } } 解除mutex的lock状态，获取到最新的状态new。如果new是饥饿状态，唤醒第一个G，获取锁。解锁完成。 如果不是饥饿状态。当前没有G等待，或者有G已经被唤醒去加锁了。就不需要做唤醒的动作。退出即可。 否则将等待的G-1，并一直尝试将G设置为唤醒状态，释放信号量，通知所有的G都可以去抢夺锁。设置成功解锁完成，否则继续执行。\n引申问题 源码阅读注意的点 在理解sync.mutex的时候，一定要注意的点是，m的state在代码执行过程中，很可能会有其他的G改变其状态。所以查看代码逻辑的时候， 一定要时刻记得这个点\nwoken状态的意义 Woken状态用于加锁和解锁过程的通信，举个例子，同一时刻，两个协程一个在加锁，一个在解锁，在加锁的协程可能在自旋过程中， 此时把Woken标记为1，用于通知解锁协程不必释放信号量了，好比在说：你只管解锁好了，不必释放信号量，我马上就拿到锁了。\n为什么重复解锁会panic Unlock的逻辑是，解除mutex的lock状态，然后检查是否有协程等待，有的话释放信号量，唤醒协程。如果多次unlock的话，就会 发送多个信号量，唤醒多个G去抢夺锁。会造成不必要的竞争，也会造成协程切换，浪费资源，实现复杂度也会增加。\nG的饥饿和mutex饥饿的关系 只有G处于饥饿状态的时候，才会将mutex设为饥饿状态。当mutex处于饥饿状态时，才可能会让饥饿的G获取到锁。需要注意的是，设mutex 为饥饿状态的G不一定会获取到锁，有可能会被其他G截胡。\nG可以成功获取锁的情况 第一次加锁的时候m.state=0，一定是可以获取锁。没有其他的G获取锁，没有改变其状态。 当前的mutex不是饥饿状态，也不是lock状态，尝试CAS加锁的时候，如果没有其他G改变m状态，可以成功。 某个G被唤醒后，重新获取mutex，此时mutex处于饥饿状态，没有其他的G来抢夺，因为这个时候只唤醒了饥饿的G，G也可以成功。 参考资料 https://blog.csdn.net/baolingye/article/details/111357407 https://blog.csdn.net/qq_37102984/article/details/115322706 https://mp.weixin.qq.com/s/BZvfNn_Vre7o2T8BZ4LLMw ","permalink":"https://younger027.github.io/en/posts/tech/go-sync.mutex/","summary":"开端 今天学习下go里面的sync.mutex的实现以及相关扩展知识。 锁的介绍 首先，计算机中的锁是为了控制并发情况下，对同一资源的并发访问。锁","title":"Go sync.mutex源码解析"},{"content":"hi,我是younger，go后端开发。\n为什么建立自己Blog 人这一生呢，生不带来，去不带走的。总得留点东西在这个世界上，证明你来过。\n","permalink":"https://younger027.github.io/en/posts/life/me/","summary":"hi,我是younger，go后端开发。 为什么建立自己Blog 人这一生呢，生不带来，去不带走的。总得留点东西在这个世界上，证明你来过。","title":"Me"},{"content":"1.go-channel阻塞的场景\n无缓冲的channel，同一个协程内读写，会导致all goroutine are asleep.dead lock 无缓冲的channel，通道的同步写早于读channel 从一个没有数据的channel里拿数据引起的死锁 循环等待引起的死锁，两个G互相持有对方拥有的资源，无法读写 有缓冲区，收发在同一个G，但是缓冲区已满，写阻塞 有缓冲区，读空的channel，读阻塞，可以加select控制 2.读写channel哪个先关。\n关闭channel的原则，不要让receiver来关闭chan。也不要在多个sender的时候由sender关闭chan。会导致panic的情况有两个。一个是给已经关闭的chan写数据。另一个是重复close chan会导致panic。一些常见的方式，可以使用sync.Once和sync.mutex来关闭chan。还可以直接用panic和recovery来处理panic。\n一读一写(只要一个写都可以写端关闭)。写端关闭。不写数据的时候，关闭chan，通知读端。\n一个读多个写。读端通知写端关闭。可以新加个close chan，通知写端不要输入了。\n多读多写。增加toStop chan去通知关闭close chan。读写端会有select检查close chan的状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 func main() { rand.Seed(time.Now().UnixNano()) log.SetFlags(0) // ... const MaxRandomNumber = 100000 const NumReceivers = 10 const NumSenders = 1000 wgReceivers := sync.WaitGroup{} wgReceivers.Add(NumReceivers) // ... dataCh := make(chan int, 100) stopCh := make(chan struct{}) // stopCh is an additional signal channel. // Its sender is the moderator goroutine shown below. // Its reveivers are all senders and receivers of dataCh. toStop := make(chan string, 1) // the channel toStop is used to notify the moderator // to close the additional signal channel (stopCh). // Its senders are any senders and receivers of dataCh. // Its reveiver is the moderator goroutine shown below. var stoppedBy string // moderator go func() { stoppedBy = \u0026lt;-toStop // part of the trick used to notify the moderator // to close the additional signal channel. close(stopCh) }() // senders for i := 0; i \u0026lt; NumSenders; i++ { go func(id string) { for { Val := rand.Intn(MaxRandomNumber) if Val == 0 { // here, a trick is used to notify the moderator // to close the additional signal channel. select { case toStop \u0026lt;- \u0026#34;sender#\u0026#34; + id: default: } return } // the first select here is to try to exit the // goroutine as early as possible. select { case \u0026lt;-stopCh: return default: } select { case \u0026lt;-stopCh: return case dataCh \u0026lt;- Val: } } }(strconv.Itoa(i)) } // receivers for i := 0; i \u0026lt; NumReceivers; i++ { go func(id string) { defer wgReceivers.Done() for { // same as senders, the first select here is to // try to exit the goroutine as early as possible. select { case \u0026lt;-stopCh: return default: } select { case \u0026lt;-stopCh: return case Val := \u0026lt;-dataCh: if Val == MaxRandomNumber-1 { // the same trick is used to notify the moderator // to close the additional signal channel. select { case toStop \u0026lt;- \u0026#34;receiver#\u0026#34; + id: default: } return } log.Println(Val) } } }(strconv.Itoa(i)) } // ... wgReceivers.Wait() log.Println(\u0026#34;stopped by\u0026#34;, stoppedBy) } ","permalink":"https://younger027.github.io/en/posts/tech/go-interview/","summary":"1.go-channel阻塞的场景 无缓冲的channel，同一个协程内读写，会导致all goroutine are asleep.dead lock 无缓冲的channel，通道的同步写早于读c","title":""}]